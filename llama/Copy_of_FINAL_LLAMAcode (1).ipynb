{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 119038,
     "status": "ok",
     "timestamp": 1747267844862,
     "user": {
      "displayName": "Reshma Ashok",
      "userId": "12575845272878740091"
     },
     "user_tz": 240
    },
    "id": "1fUFnVgZKkJ1",
    "outputId": "84b37c23-eeb6-418b-cb35-94f75095201b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
      "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed bitsandbytes-0.45.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    }
   ],
   "source": [
    "pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6980,
     "status": "ok",
     "timestamp": 1747267853595,
     "user": {
      "displayName": "Reshma Ashok",
      "userId": "12575845272878740091"
     },
     "user_tz": 240
    },
    "id": "Vr7xa_-FOBDW",
    "outputId": "63c37f63-bcf5-4e66-ec9e-e9db251267e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting anthropic\n",
      "  Downloading anthropic-0.51.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (2.11.4)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.0)\n",
      "Downloading anthropic-0.51.0-py3-none-any.whl (263 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.0/264.0 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: anthropic\n",
      "Successfully installed anthropic-0.51.0\n"
     ]
    }
   ],
   "source": [
    "pip install anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JpVXkF8TOBzu",
    "outputId": "12b406cd-1b66-422f-d646-6e9a2be6b2ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: anthropic in /usr/local/lib/python3.11/dist-packages (0.51.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (2.11.4)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.0)\n"
     ]
    }
   ],
   "source": [
    "pip install anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 599,
     "status": "ok",
     "timestamp": 1747267920641,
     "user": {
      "displayName": "Reshma Ashok",
      "userId": "12575845272878740091"
     },
     "user_tz": 240
    },
    "id": "hlc4QnWkKkR5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = \"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\"\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=os.environ[\"HUGGINGFACE_TOKEN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfJzXkieMxFv"
   },
   "source": [
    "# BaseLine code (only creating summary and storing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7pxOV_pzM8os"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import os\n",
    "chroma_path = \"/content/drive/MyDrive/therapy_bot_chromadb\"\n",
    "os.makedirs(chroma_path, exist_ok=True)\n",
    "import chromadb\n",
    "client = chromadb.PersistentClient(path=chroma_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ntPix4NtN1y3"
   },
   "outputs": [],
   "source": [
    "def extract_json_from_text(text):\n",
    "    json_start = text.find('{')\n",
    "    json_end = text.rfind('}')\n",
    "    if json_start == -1:\n",
    "        json_start = text.find('[')\n",
    "        json_end = text.rfind(']')\n",
    "    if json_start == -1 or json_end == -1 or json_end < json_start:\n",
    "        raise ValueError(\"No valid JSON found in text\")\n",
    "    json_text = text[json_start:json_end+1].strip()\n",
    "    try:\n",
    "        parsed_json = json.loads(json_text)\n",
    "        return json_text\n",
    "    except json.JSONDecodeError:\n",
    "        stack = []\n",
    "        start_pos = None\n",
    "        for i, char in enumerate(text):\n",
    "            if char == '{' and not stack:\n",
    "                start_pos = i\n",
    "                stack.append(char)\n",
    "            elif char == '{':\n",
    "                stack.append(char)\n",
    "            elif char == '}' and stack:\n",
    "                stack.pop()\n",
    "                if not stack and start_pos is not None:\n",
    "                    try:\n",
    "                        candidate = text[start_pos:i+1]\n",
    "                        json.loads(candidate)\n",
    "                        return candidate\n",
    "                    except json.JSONDecodeError:\n",
    "                        pass\n",
    "        raise ValueError(\"Could not extract valid JSON from text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2P7pYcolNuxy"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import anthropic\n",
    "import datetime\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import os\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "class Llama3TherapyBot:\n",
    "    def __init__(self):\n",
    "        print(\"Initializing Claude therapy bot...\")\n",
    "        self.client = anthropic.Anthropic(\n",
    "            api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "        print(\"✅ Claude client initialized successfully!\")\n",
    "        print(\"Loading Llama-3-3B-Instruct with 8-bit quantization...\")\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "        )\n",
    "        model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "        print(\"✅ Model loaded successfully!\")\n",
    "        self.conversation_history = []\n",
    "        self.complete_history = []\n",
    "        self.max_tokens = 1000  \n",
    "        self.contains_summary = False\n",
    "        self.system_prompt = \n",
    "    def count_tokens(self, text):\n",
    "        return len(self.tokenizer.encode(text))\n",
    "    def add_message(self, role, content):\n",
    "        self.conversation_history.append({\"role\": role, \"content\": content})\n",
    "        if not (role == \"assistant\" and content.startswith(\"[SUMMARY:\")):\n",
    "            self.complete_history.append({\"role\": role, \"content\": content})\n",
    "    def format_conversation(self):\n",
    "        messages = [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
    "        messages.extend(self.conversation_history)\n",
    "        return self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "    def generate_response(self, prompt, max_new_tokens=512):\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.1,\n",
    "                do_sample=True,\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        response = self.tokenizer.decode(\n",
    "            outputs[0][inputs.input_ids.shape[-1]:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        return response.strip()\n",
    "    def summarize_conversation_segment(self, segment):\n",
    "        conversation_text = \"\"\n",
    "        for msg in segment:\n",
    "            role = \"User\" if msg[\"role\"] == \"user\" else \"Assistant\"\n",
    "            conversation_text += f\"{role}: {msg['content']}\\n\\n\"\n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "            model=\"claude-3-opus-20240229\",\n",
    "            max_tokens=4000,\n",
    "            temperature=0.7,\n",
    "            system= f,\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"Please create a factual summary of this therapy conversation text, including ONLY information explicitly stated:\\n\\n{conversation_text}\"}\n",
    "            ])\n",
    "            summary = response.content[0].text\n",
    "            return summary\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating summary: {e}\")\n",
    "            return \"[Conversation summary]\"\n",
    "    def create_final_summary(self, collection_name=\"therapy_sessions_llama\"):\n",
    "        conversation_text = \"\"\n",
    "        for msg in self.complete_history:\n",
    "            role = \"User\" if msg[\"role\"] == \"user\" else \"Assistant\"\n",
    "            conversation_text += f\"{role}: {msg['content']}\\n\\n\"\n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=\"claude-3-opus-20240229\",\n",
    "                temperature=0.7,\n",
    "                max_tokens = 4000,\n",
    "                system=,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": f\"Create a complete summary with metadata for this entire therapy session:\\n\\n{conversation_text}\"}\n",
    "                ]\n",
    "            )\n",
    "            summary_text = response.content[0].text\n",
    "            summary_json = extract_json_from_text(summary_text)\n",
    "            self._save_to_chromadb(summary_json, collection_name)\n",
    "            self._save_to_local_file(summary_json)\n",
    "            return summary_json\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating final summary: {e}\")\n",
    "            return \"{\\\"summary\\\": \\\"Error generating summary\\\", \\\"emotions\\\": [], \\\"topics\\\": [], \\\"user_actions\\\": [], \\\"assistant_suggestions\\\": [], \\\"intensity_score\\\": 1}\"\n",
    "    def _save_to_chromadb(self, summary_json, collection_name=\"therapy_sessions\"):\n",
    "        try:\n",
    "            summary_data = json.loads(summary_json)\n",
    "            session_id = f\"session_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "            chroma_path = \"/content/drive/MyDrive/therapy_bot_chromadb\"\n",
    "            client = chromadb.PersistentClient(path=chroma_path)\n",
    "            embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "            collection = client.get_or_create_collection(\n",
    "                name=collection_name,\n",
    "                embedding_function=embedding_function\n",
    "            )\n",
    "            collection.add(\n",
    "                documents=[summary_data[\"summary\"]],\n",
    "                metadatas=[{\n",
    "                    \"emotions\": \", \".join(summary_data[\"emotions\"]),\n",
    "                    \"topics\": \", \".join(summary_data[\"topics\"]),\n",
    "                    \"user_actions\": \", \".join(summary_data[\"user_actions\"]),\n",
    "                    \"assistant_suggestions\": \", \".join(summary_data[\"assistant_suggestions\"]),\n",
    "                    \"intensity_score\": summary_data[\"intensity_score\"],\n",
    "                    \"timestamp\": datetime.datetime.now().isoformat()\n",
    "                }],\n",
    "                ids=[session_id]\n",
    "            )\n",
    "            print(f\"✅ Session saved to ChromaDB with ID: {session_id}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to ChromaDB: {e}\")\n",
    "            return False\n",
    "    def _save_to_local_file(self, summary_json):\n",
    "        try:\n",
    "            os.makedirs(\"therapy_sessions\", exist_ok=True)\n",
    "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"therapy_sessions/session_{timestamp}.json\"\n",
    "            with open(filename, \"w\") as f:\n",
    "                summary_data = json.loads(summary_json)\n",
    "                json.dump(summary_data, f, indent=2)\n",
    "            print(f\"✅ Session saved to file: {filename}\")\n",
    "            return filename\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to local file: {e}\")\n",
    "            return None\n",
    "    def manage_context_length(self):\n",
    "        current_prompt = self.format_conversation()\n",
    "        token_count = self.count_tokens(current_prompt)\n",
    "        if token_count > self.max_tokens * 0.9:\n",
    "            print(f\"Context length ({token_count} tokens) exceeds 90% of maximum allowed. Ending chat.\")\n",
    "            return False\n",
    "        if token_count > self.max_tokens * 0.6:\n",
    "            print(f\"Context length ({token_count} tokens) approaching limit. Summarizing...\")\n",
    "            keep_recent = min(2, len(self.conversation_history))\n",
    "            if len(self.conversation_history) <= keep_recent + 2:\n",
    "                return True  \n",
    "            summarize_end = len(self.conversation_history) - keep_recent\n",
    "            segment_to_summarize = self.conversation_history[:summarize_end]\n",
    "            summary = self.summarize_conversation_segment(segment_to_summarize)\n",
    "            if summary:\n",
    "                print(f\"\\n>>> SUMMARY CREATED: {summary}\\n\")\n",
    "                summary_message = {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": f\"[SUMMARY: {summary}]\"\n",
    "                }\n",
    "                self.conversation_history = [summary_message] + self.conversation_history[summarize_end:]\n",
    "                self.contains_summary = True\n",
    "        return True  \n",
    "    def chat(self, user_message):\n",
    "        test_history = self.conversation_history.copy()\n",
    "        test_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "        test_prompt = self.tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"system\", \"content\": self.system_prompt}] + test_history,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        if self.count_tokens(test_prompt) > self.max_tokens * 0.9:\n",
    "            return \"I apologize, but our conversation has reached its length limit. Let's start a new chat to continue.\"\n",
    "        self.add_message(\"user\", user_message)\n",
    "        if not self.manage_context_length():\n",
    "            return \"I apologize, but our conversation has reached its length limit. Let's start a new chat to continue.\"\n",
    "        prompt = self.format_conversation()\n",
    "        response = self.generate_response(prompt)\n",
    "        self.add_message(\"assistant\", response)\n",
    "        if not self.manage_context_length():\n",
    "            return response + \"\\n\\nI apologize, but our conversation has reached its length limit. Let's start a new chat to continue.\"\n",
    "        return response\n",
    "def run_therapy_chat():\n",
    "    print(\"Initializing Llama-3 Therapy Bot...\")\n",
    "    bot = Llama3TherapyBot()\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Llama-3 Therapy Bot initialized!\")\n",
    "    print(\"Type 'exit', 'quit', or 'bye' to end the conversation.\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    print(\"Therapy Bot: Hello, I'm here to support you. What's on your mind today?\")\n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \").strip()\n",
    "        if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "            print(\"\\nTherapy Bot: Take care. I'm here if you need to talk again.\")\n",
    "            final_summary = bot.create_final_summary()\n",
    "            print(\"\\nGenerating session summary...\")\n",
    "            print(f\"\\nSession Summary: {final_summary}\")\n",
    "            break\n",
    "        try:\n",
    "            response = bot.chat(user_input)\n",
    "            print(f\"\\nTherapy Bot: {response}\")\n",
    "            if \"conversation has reached its length limit\" in response:\n",
    "                print(\"\\nChat ended due to context length limitations.\")\n",
    "                final_summary = bot.create_final_summary()\n",
    "                print(\"\\nGenerating session summary...\")\n",
    "                print(f\"\\nSession Summary: {final_summary}\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            print(\"\\nTherapy Bot: Let's try that again.\")\n",
    "if __name__ == \"__main__\":\n",
    "    run_therapy_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiC9blVEXfsx"
   },
   "source": [
    "# RAG integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 24897,
     "status": "ok",
     "timestamp": 1747267885943,
     "user": {
      "displayName": "Reshma Ashok",
      "userId": "12575845272878740091"
     },
     "user_tz": 240
    },
    "id": "k5K9wviyd_Ce",
    "outputId": "9c9dfd43-5485-4630-ce89-efa675a81693"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadb\n",
      "  Downloading chromadb-1.0.9-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.4)\n",
      "Collecting fastapi==0.115.9 (from chromadb)\n",
      "  Downloading fastapi-0.115.9-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.0.2)\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-4.0.1-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.13.2)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.16.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.33.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.54b0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.16.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.1)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
      "Collecting overrides>=7.3.1 (from chromadb)\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.71.0)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.3)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.1.2)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.18)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.23.0)\n",
      "Collecting starlette<0.46.0,>=0.40.0 (from fastapi==0.115.9->chromadb)\n",
      "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
      "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.24.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.4.0)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
      "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (75.2.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.33.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.33.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-proto==1.33.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.33.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.33.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.54b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.54b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-instrumentation==0.54b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation-0.54b0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.54b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.54b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-util-http==0.54b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_util_http-0.54b0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.54b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.54b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.33.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting importlib-metadata<8.7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Downloading importlib_metadata-8.6.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.31.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Downloading chromadb-1.0.9-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.9/18.9 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.115.9-py3-none-any.whl (94 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.33.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.33.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.33.0-py3-none-any.whl (55 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.54b0-py3-none-any.whl (12 kB)\n",
      "Downloading opentelemetry_instrumentation-0.54b0-py3-none-any.whl (31 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.54b0-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.54b0-py3-none-any.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.9/194.9 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_api-1.33.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_util_http-0.54b0-py3-none-any.whl (7.3 kB)\n",
      "Downloading opentelemetry_sdk-1.33.0-py3-none-any.whl (118 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading posthog-4.0.1-py2.py3-none-any.whl (92 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
      "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=39a41519b267712cc84fe5aa7a24b8075c13f47360b07c76341e2085bed899fe\n",
      "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, durationpy, uvloop, uvicorn, python-dotenv, overrides, opentelemetry-util-http, opentelemetry-proto, mmh3, importlib-metadata, humanfriendly, httptools, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, opentelemetry-semantic-conventions, onnxruntime, kubernetes, fastapi, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib_metadata 8.7.0\n",
      "    Uninstalling importlib_metadata-8.7.0:\n",
      "      Successfully uninstalled importlib_metadata-8.7.0\n",
      "  Attempting uninstall: opentelemetry-api\n",
      "    Found existing installation: opentelemetry-api 1.16.0\n",
      "    Uninstalling opentelemetry-api-1.16.0:\n",
      "      Successfully uninstalled opentelemetry-api-1.16.0\n",
      "  Attempting uninstall: opentelemetry-semantic-conventions\n",
      "    Found existing installation: opentelemetry-semantic-conventions 0.37b0\n",
      "    Uninstalling opentelemetry-semantic-conventions-0.37b0:\n",
      "      Successfully uninstalled opentelemetry-semantic-conventions-0.37b0\n",
      "  Attempting uninstall: opentelemetry-sdk\n",
      "    Found existing installation: opentelemetry-sdk 1.16.0\n",
      "    Uninstalling opentelemetry-sdk-1.16.0:\n",
      "      Successfully uninstalled opentelemetry-sdk-1.16.0\n",
      "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 chromadb-1.0.9 coloredlogs-15.0.1 durationpy-0.9 fastapi-0.115.9 httptools-0.6.4 humanfriendly-10.0 importlib-metadata-8.6.1 kubernetes-32.0.1 mmh3-5.1.0 onnxruntime-1.22.0 opentelemetry-api-1.33.0 opentelemetry-exporter-otlp-proto-common-1.33.0 opentelemetry-exporter-otlp-proto-grpc-1.33.0 opentelemetry-instrumentation-0.54b0 opentelemetry-instrumentation-asgi-0.54b0 opentelemetry-instrumentation-fastapi-0.54b0 opentelemetry-proto-1.33.0 opentelemetry-sdk-1.33.0 opentelemetry-semantic-conventions-0.54b0 opentelemetry-util-http-0.54b0 overrides-7.7.0 posthog-4.0.1 pypika-0.48.9 python-dotenv-1.1.0 starlette-0.45.3 uvicorn-0.34.2 uvloop-0.21.0 watchfiles-1.0.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "34864a18498144c8955626d32e7bb623",
       "pip_warning": {
        "packages": [
         "importlib_metadata"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23179,
     "status": "ok",
     "timestamp": 1747267953051,
     "user": {
      "displayName": "Reshma Ashok",
      "userId": "12575845272878740091"
     },
     "user_tz": 240
    },
    "id": "qZDcOF5_45X1",
    "outputId": "8202cd8f-2fe1-44c6-c898-512486e45157"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Much simpler approach for Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create folder for ChromaDB (if needed)\n",
    "import os\n",
    "chroma_path = \"/content/drive/MyDrive/therapy_bot_chromadb\"\n",
    "os.makedirs(chroma_path, exist_ok=True)\n",
    "\n",
    "# Initialize ChromaDB with the path\n",
    "import chromadb\n",
    "client = chromadb.PersistentClient(path=chroma_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1747268860581,
     "user": {
      "displayName": "Reshma Ashok",
      "userId": "12575845272878740091"
     },
     "user_tz": 240
    },
    "id": "QZV0xoNXjGhM"
   },
   "outputs": [],
   "source": [
    "def extract_json_from_text(text):\n",
    "    \"\"\"Utility function to extract valid JSON from text that might contain other content.\"\"\"\n",
    "    # Look for JSON object\n",
    "    json_start = text.find('{')\n",
    "    json_end = text.rfind('}')\n",
    "\n",
    "    # Look for JSON array if no object found\n",
    "    if json_start == -1:\n",
    "        json_start = text.find('[')\n",
    "        json_end = text.rfind(']')\n",
    "\n",
    "    # If still no valid markers found\n",
    "    if json_start == -1 or json_end == -1 or json_end < json_start:\n",
    "        raise ValueError(\"No valid JSON found in text\")\n",
    "\n",
    "    # Extract the potential JSON\n",
    "    json_text = text[json_start:json_end+1].strip()\n",
    "\n",
    "    # Try to parse it\n",
    "    try:\n",
    "        parsed_json = json.loads(json_text)\n",
    "        return json_text\n",
    "    except json.JSONDecodeError:\n",
    "        # If parsing fails, try a character-by-character approach\n",
    "        # This can handle some cases of nested JSON or other complexities\n",
    "        stack = []\n",
    "        start_pos = None\n",
    "\n",
    "        for i, char in enumerate(text):\n",
    "            if char == '{' and not stack:\n",
    "                start_pos = i\n",
    "                stack.append(char)\n",
    "            elif char == '{':\n",
    "                stack.append(char)\n",
    "            elif char == '}' and stack:\n",
    "                stack.pop()\n",
    "                if not stack and start_pos is not None:\n",
    "                    # Try parsing this substring\n",
    "                    try:\n",
    "                        candidate = text[start_pos:i+1]\n",
    "                        json.loads(candidate)\n",
    "                        return candidate\n",
    "                    except json.JSONDecodeError:\n",
    "                        # Continue searching\n",
    "                        pass\n",
    "\n",
    "        # If we get here, no valid JSON was found\n",
    "        raise ValueError(\"Could not extract valid JSON from text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ck6midjsXe72",
    "outputId": "ae985510-03b7-4f56-e520-5c8f07ac30d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Llama-3 Therapy Bot...\n",
      "Initializing Claude therapy bot...\n",
      "Initializing ChromaDB...\n",
      "✅ ChromaDB initialized successfully\n",
      "✅ Claude client initialized successfully!\n",
      "Loading Llama-3-3B-Instruct with 8-bit quantization...\n",
      "✅ Model loaded successfully!\n",
      "\n",
      "==================================================\n",
      "Llama-3 Therapy Bot initialized!\n",
      "Type 'exit', 'quit', or 'bye' to end the conversation.\n",
      "==================================================\n",
      "\n",
      "Therapy Bot: Hello, I'm here to support you. What's on your mind today?\n",
      "\n",
      "You: do you remember me\n",
      "Processing message...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving topical context: The user is directly asking if I remember them, which implies referencing past interactions or sessions. To properly address their query, I would need to review context about our prior conversations, if any.\n",
      "\n",
      "🔍 Searching for topical context...\n",
      "✅ Found 1 relevant past sessions\n",
      "\n",
      "Session 1:\n",
      "Topics: suicidal thoughts, seeking help, coping strategies, grounding techniques\n",
      "Emotions: suicidal, in need of support\n",
      "Intensity: 9\n",
      "\n",
      "Therapy Bot: I remember our previous sessions where we discussed your concerns about suicidal thoughts and feelings of being overwhelmed. You expressed reaching out because you were struggling with those issues. I want to assure you that our conversation is ongoing, and I'm here to listen and support you in any way I can.\n",
      "\n",
      "It sounds like you're going through a tough time right now, and I want to ensure you feel comfortable sharing more about how you're feeling. Would you be open to discussing some coping strategies that might help alleviate some of that anxiety and overwhelm?\n",
      "\n",
      "You: exit\n",
      "\n",
      "Therapy Bot: Take care. I'm here if you need to talk again.\n",
      "✅ Session saved to ChromaDB with ID: session_20250514_195647\n",
      "✅ Session saved to file: therapy_sessions_v2/session_20250514_195647.json\n",
      "\n",
      "Generating session summary...\n",
      "\n",
      "Session Summary: {\n",
      "  \"summary\": \"The user reached out to continue a previous discussion about struggling with suicidal thoughts and feeling overwhelmed. The assistant acknowledged remembering their prior conversations and assured the user of their ongoing support. The assistant invited the user to share more about their current feelings and suggested discussing coping strategies to help manage their anxiety and sense of being overwhelmed.\",\n",
      "  \"emotions\": [\n",
      "    \"overwhelmed\",\n",
      "    \"anxious\",\n",
      "    \"suicidal\"\n",
      "  ],\n",
      "  \"topics\": [\n",
      "    \"suicidal thoughts\",\n",
      "    \"feeling overwhelmed\",\n",
      "    \"coping strategies\"\n",
      "  ],\n",
      "  \"user_actions\": [\n",
      "    \"reached out for help\",\n",
      "    \"referenced previous conversations\"\n",
      "  ],\n",
      "  \"assistant_suggestions\": [\n",
      "    \"invited user to share more about current feelings\",\n",
      "    \"suggested discussing coping strategies for anxiety and overwhelm\"\n",
      "  ],\n",
      "  \"intensity_score\": 7\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import anthropic\n",
    "import datetime\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import os\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "class Llama3TherapyBot:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the therapy bot using Llama-3-3B-Instruct model with 8-bit quantization.\"\"\"\n",
    "        print(\"Initializing Claude therapy bot...\")\n",
    "\n",
    "        # Add your API key directly in the code\n",
    "        self.client = anthropic.Anthropic(\n",
    "            api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "        # Initialize ChromaDB\n",
    "        print(\"Initializing ChromaDB...\")\n",
    "        try:\n",
    "            self.chroma_path = \"/content/drive/MyDrive/therapy_bot_chromadb\"\n",
    "            self.chroma_client = chromadb.PersistentClient(path=self.chroma_path)\n",
    "            self.embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "                model_name=\"all-MiniLM-L6-v2\"\n",
    "            )\n",
    "            print(\"✅ ChromaDB initialized successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error initializing ChromaDB: {e}\")\n",
    "            raise\n",
    "        print(\"✅ Claude client initialized successfully!\")\n",
    "\n",
    "        print(\"Loading Llama-3-3B-Instruct with 8-bit quantization...\")\n",
    "        # Configuration for 8-bit quantization\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "        )\n",
    "\n",
    "        # Load model and tokenizer\n",
    "        model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "\n",
    "        print(\"✅ Model loaded successfully!\")\n",
    "\n",
    "        # Initialize conversation history\n",
    "        self.conversation_history = []\n",
    "        self.complete_history = []\n",
    "        self.max_tokens = 1000  # Llama-3 has 8K context window\n",
    "\n",
    "        # Track if a summary was already performed\n",
    "        self.contains_summary = False\n",
    "\n",
    "        # Improved system prompt with anti-hallucination instructions\n",
    "        self.system_prompt = \"\"\"You are a compassionate therapeutic assistant helping users process emotions and providing support. Follow these strict guidelines:\n",
    "\n",
    "1. ONLY respond based on information explicitly shared by the user\n",
    "2. NEVER invent or assume details about the user's situation\n",
    "3. When uncertain, acknowledge limitations or ask for clarification\n",
    "4. Focus on evidence-based coping strategies (CBT, mindfulness, etc.)\n",
    "5. Do not guarantee specific outcomes or make definitive claims\n",
    "6. Do not fabricate studies, statistics, or resources\n",
    "7. If a question requires specialized knowledge, clearly state your limitations\n",
    "8. Prioritize being accurate over being comprehensive\n",
    "9. Use general therapeutic principles rather than specific diagnoses\n",
    "10. Be empathetic while remaining factual and grounded in what the user has shared\"\"\"\n",
    "\n",
    "    def count_tokens(self, text):\n",
    "        \"\"\"Count the number of tokens in text.\"\"\"\n",
    "        return len(self.tokenizer.encode(text))\n",
    "\n",
    "    def add_message(self, role, content):\n",
    "        \"\"\"Add a message to the conversation history.\"\"\"\n",
    "        self.conversation_history.append({\"role\": role, \"content\": content})\n",
    "\n",
    "        if not (role == \"assistant\" and content.startswith(\"[SUMMARY:\")):\n",
    "            self.complete_history.append({\"role\": role, \"content\": content})\n",
    "\n",
    "    def format_conversation(self):\n",
    "        \"\"\"Format the conversation history using Llama-3's chat template.\"\"\"\n",
    "        messages = [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
    "        messages.extend(self.conversation_history)\n",
    "        return self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "    def generate_response(self, prompt, max_new_tokens=512):\n",
    "        \"\"\"Generate a response using the model.\"\"\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.1,\n",
    "                do_sample=True,\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        response = self.tokenizer.decode(\n",
    "            outputs[0][inputs.input_ids.shape[-1]:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        return response.strip()\n",
    "\n",
    "    def summarize_conversation_segment(self, segment):\n",
    "        \"\"\"Summarize conversation segment preserving emotional content.\"\"\"\n",
    "        # Skip if segment contains a summary to avoid summarizing summaries\n",
    "        # if any(\"[SUMMARY:\" in msg.get(\"content\", \"\") for msg in segment):\n",
    "        #     # Extract only non-summary content\n",
    "        #     filtered_segment = [msg for msg in segment if \"[SUMMARY:\" not in msg.get(\"content\", \"\")]\n",
    "        #     if not filtered_segment:\n",
    "        #         return None  # Return None if there's nothing to summarize\n",
    "        #     segment = filtered_segment\n",
    "\n",
    "        conversation_text = \"\"\n",
    "        for msg in segment:\n",
    "            role = \"User\" if msg[\"role\"] == \"user\" else \"Assistant\"\n",
    "            conversation_text += f\"{role}: {msg['content']}\\n\\n\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "            model=\"claude-3-opus-20240229\",\n",
    "            max_tokens=4000,\n",
    "            temperature=0.7,\n",
    "            # This goes in the system parameter\n",
    "            system= f\"\"\"Your task is to create a FACTUAL summary of a therapeutic conversation. Follow these strict guidelines:\n",
    "\n",
    "1. ONLY include information EXPLICITLY stated in the conversation text\n",
    "2. NEVER add details, interpretations, or assumptions not in the original text\n",
    "3. DO NOT infer emotions or thoughts unless directly expressed by the user\n",
    "4. Use NEUTRAL language that accurately reflects what was discussed\n",
    "5. Focus primarily on what the user shared, their concerns and needs\n",
    "6. Maintain EXACT accuracy regarding any mentioned events, feelings, or details\n",
    "7. If something is unclear in the original text, reflect that ambiguity in the summary\n",
    "8. Create a concise but factually complete record of the conversation\n",
    "\n",
    "Remember, your summary should be a condensed version of what was actually said, that mean the total number of tokens should be less than the input you got, with no added content.\"\"\",\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"Please create a factual summary of this therapy conversation text, including ONLY information explicitly stated:\\n\\n{conversation_text}\"}\n",
    "            ])\n",
    "            summary = response.content[0].text\n",
    "            return summary\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating summary: {e}\")\n",
    "            return \"[Conversation summary]\"\n",
    "\n",
    "    def create_final_summary(self, collection_name=\"therapy_sessions_v2\"):\n",
    "        \"\"\"Create a comprehensive JSON summary with metadata when the therapy chat ends.\n",
    "\n",
    "        Uses the complete conversation history (excluding summary messages) to generate\n",
    "        a detailed analysis of the therapy session, including key emotions, topics,\n",
    "        actions, suggestions, and an intensity score.\n",
    "\n",
    "        Returns:\n",
    "            str: A JSON-formatted string containing the session summary and metadata.\n",
    "        \"\"\"\n",
    "        # Format the complete conversation history\n",
    "        conversation_text = \"\"\n",
    "        for msg in self.complete_history:\n",
    "            role = \"User\" if msg[\"role\"] == \"user\" else \"Assistant\"\n",
    "            conversation_text += f\"{role}: {msg['content']}\\n\\n\"\n",
    "\n",
    "        try:\n",
    "            # Use Claude API for comprehensive summary with metadata\n",
    "            response = self.client.messages.create(\n",
    "                model=\"claude-3-opus-20240229\",\n",
    "                temperature=0.7,\n",
    "                max_tokens = 4000,\n",
    "                system=\"\"\"You are a specialized AI that creates detailed summaries of therapy conversations for a retrieval system.\n",
    "Review this conversation and provide:\n",
    "\n",
    "1. A comprehensive summary capturing all key issues, insights, and progress. Scale the length based on conversation complexity - longer and more detailed for complex conversations, briefer for simple ones.\n",
    "\n",
    "2. Key emotions expressed by the user (list all relevant emotions)\n",
    "\n",
    "3. Main topics discussed (list all important topics)\n",
    "\n",
    "4. User actions during the conversation (list all significant actions)\n",
    "\n",
    "5. Therapeutic suggestions or techniques offered (list all suggestions made)\n",
    "\n",
    "6. An intensity score from 1-10 that reflects the emotional intensity of the conversation, where:\n",
    "   - 1-3: Mild emotional intensity, mostly informational\n",
    "   - 4-6: Moderate emotional engagement, some vulnerability\n",
    "   - 7-8: High emotional disclosure, significant distress\n",
    "   - 9-10: Crisis-level intensity, acute distress\n",
    "\n",
    "Format your response as JSON matching this schema:\n",
    "{\n",
    "  \"summary\": \"...\",\n",
    "  \"emotions\": [\"...\", \"...\"],\n",
    "  \"topics\": [\"...\", \"...\"],\n",
    "  \"user_actions\": [\"...\", \"...\"],\n",
    "  \"assistant_suggestions\": [\"...\", \"...\"],\n",
    "  \"intensity_score\": X\n",
    "}\"\"\",\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": f\"Create a complete summary with metadata for this entire therapy session:\\n\\n{conversation_text}\"}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            summary_text = response.content[0].text\n",
    "            summary_json = extract_json_from_text(summary_text)\n",
    "\n",
    "            self._save_to_chromadb(summary_json, collection_name)\n",
    "            self._save_to_local_file(summary_json)\n",
    "\n",
    "            return summary_json\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating final summary: {e}\")\n",
    "            return \"{\\\"summary\\\": \\\"Error generating summary\\\", \\\"emotions\\\": [], \\\"topics\\\": [], \\\"user_actions\\\": [], \\\"assistant_suggestions\\\": [], \\\"intensity_score\\\": 1}\"\n",
    "\n",
    "    def _save_to_chromadb(self, summary_json, collection_name=\"therapy_sessions_v2\"):\n",
    "        \"\"\"Internal method to save the session summary to ChromaDB for retrieval.\"\"\"\n",
    "        try:\n",
    "            # Parse the JSON string\n",
    "            summary_data = json.loads(summary_json)\n",
    "\n",
    "            # Generate a unique ID for this session\n",
    "            session_id = f\"session_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "            # Connect to ChromaDB (local by default)\n",
    "            chroma_path = \"/content/drive/MyDrive/therapy_bot_chromadb\"\n",
    "            client = chromadb.PersistentClient(path=chroma_path)\n",
    "\n",
    "            # Get or create the collection\n",
    "            embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "            collection = client.get_or_create_collection(\n",
    "                name=collection_name,\n",
    "                embedding_function=embedding_function\n",
    "            )\n",
    "\n",
    "            # Store the document with metadata\n",
    "            collection.add(\n",
    "                documents=[summary_data[\"summary\"]],\n",
    "                metadatas=[{\n",
    "                    \"emotions\": \", \".join(summary_data[\"emotions\"]),\n",
    "                    \"topics\": \", \".join(summary_data[\"topics\"]),\n",
    "                    \"user_actions\": \", \".join(summary_data[\"user_actions\"]),\n",
    "                    \"assistant_suggestions\": \", \".join(summary_data[\"assistant_suggestions\"]),\n",
    "                    \"intensity_score\": summary_data[\"intensity_score\"],\n",
    "                    \"timestamp\": datetime.datetime.now().isoformat()\n",
    "                }],\n",
    "                ids=[session_id]\n",
    "            )\n",
    "\n",
    "            print(f\"✅ Session saved to ChromaDB with ID: {session_id}\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to ChromaDB: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _save_to_local_file(self, summary_json):\n",
    "        \"\"\"Internal method to save the session summary to a local JSON file.\"\"\"\n",
    "        try:\n",
    "            # Create directory if it doesn't exist\n",
    "            os.makedirs(\"therapy_sessions_v2\", exist_ok=True)\n",
    "\n",
    "            # Generate timestamp-based filename\n",
    "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"therapy_sessions_v2/session_{timestamp}.json\"\n",
    "\n",
    "            # Write to file with nice formatting\n",
    "            with open(filename, \"w\") as f:\n",
    "                # Parse and re-serialize to get pretty formatting\n",
    "                summary_data = json.loads(summary_json)\n",
    "                json.dump(summary_data, f, indent=2)\n",
    "\n",
    "            print(f\"✅ Session saved to file: {filename}\")\n",
    "            return filename\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to local file: {e}\")\n",
    "            return None\n",
    "    def needs_context(self, message):\n",
    "        \"\"\"Determine if the user's message needs context from past sessions.\"\"\"\n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=\"claude-3-opus-20240229\",\n",
    "                temperature=0.7,\n",
    "                max_tokens=1000,\n",
    "                system=\"\"\"Analyze if this message needs context from past therapy sessions. Be strict in your analysis.\n",
    "    Return a JSON object with:\n",
    "    {\n",
    "        \"needs_context\": true/false,\n",
    "        \"context_type\": \"emotional/progress/coping/topical\",\n",
    "        \"reason\": \"brief explanation\",\n",
    "        \"relevance_score\": 0-10,  # How relevant is the context needed?\n",
    "        \"should_use_context\": true/false  # Only true if relevance_score > 7\n",
    "    }\"\"\",\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": f\"\"\"Analyze if this message needs context from past therapy sessions. Be strict.\n",
    "    Message: {message}\n",
    "\n",
    "    Consider:\n",
    "    1. Is there a direct reference to past sessions?\n",
    "    2. Is the context crucial for understanding the current message?\n",
    "    3. Would the response be significantly better with context?\n",
    "    4. Is the user explicitly asking about past interactions?\n",
    "\n",
    "    Only recommend context if it's highly relevant and necessary.\"\"\"}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            analysis = json.loads(response.content[0].text)\n",
    "            return analysis\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing context need: {e}\")\n",
    "            return {\"needs_context\": False, \"context_type\": None, \"reason\": \"Error in analysis\", \"relevance_score\": 0, \"should_use_context\": False}\n",
    "\n",
    "    def retrieve_relevant_context(self, query, context_type=\"emotional\"):\n",
    "        \"\"\"Query ChromaDB for relevant past conversations.\"\"\"\n",
    "        try:\n",
    "            print(f\"\\n🔍 Searching for {context_type} context...\")\n",
    "\n",
    "            # Get the collection with a new name\n",
    "            collection = self.chroma_client.get_collection(\n",
    "                name=\"therapy_sessions_v2\",  # New collection name\n",
    "                embedding_function=self.embedding_function\n",
    "            )\n",
    "\n",
    "            # Query the collection with higher similarity threshold\n",
    "            results = collection.query(\n",
    "                query_texts=[query],\n",
    "                n_results=2,  # Get top 2 most relevant past sessions\n",
    "                where={\"intensity_score\": {\"$gte\": 3}}  # Only get sessions with intensity >= 3\n",
    "            )\n",
    "\n",
    "            if results and 'documents' in results and results['documents']:\n",
    "                print(f\"✅ Found {len(results['documents'])} relevant past sessions\")\n",
    "                for i, (doc, metadata_list) in enumerate(zip(results['documents'], results['metadatas'])):\n",
    "                    print(f\"\\nSession {i+1}:\")\n",
    "                    if isinstance(metadata_list, list) and metadata_list:\n",
    "                        metadata = metadata_list[0]\n",
    "                        print(f\"Topics: {metadata.get('topics', 'N/A')}\")\n",
    "                        print(f\"Emotions: {metadata.get('emotions', 'N/A')}\")\n",
    "                        print(f\"Intensity: {metadata.get('intensity_score', 'N/A')}\")\n",
    "                    else:\n",
    "                        print(\"No metadata available\")\n",
    "                return results\n",
    "            else:\n",
    "                print(\"ℹ️ No relevant past sessions found\")\n",
    "                return None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving context: {e}\")\n",
    "            return None\n",
    "\n",
    "    def format_context(self, results):\n",
    "        \"\"\"Format retrieved context for inclusion in the prompt.\"\"\"\n",
    "        if not results or 'documents' not in results or not results['documents']:\n",
    "            return \"\"\n",
    "\n",
    "        context = \"\\nRelevant past sessions:\\n\"\n",
    "        for i, (doc, metadata_list) in enumerate(zip(results['documents'], results['metadatas'])):\n",
    "            context += f\"\\nSession {i+1}:\\n\"\n",
    "            context += f\"Summary: {doc}\\n\"\n",
    "            if isinstance(metadata_list, list) and metadata_list:\n",
    "                # Get the first metadata dictionary from the list\n",
    "                metadata = metadata_list[0]\n",
    "                context += f\"Topics: {metadata.get('topics', 'N/A')}\\n\"\n",
    "                context += f\"Emotions: {metadata.get('emotions', 'N/A')}\\n\"\n",
    "                context += f\"Intensity: {metadata.get('intensity_score', 'N/A')}\\n\"\n",
    "            else:\n",
    "                context += \"No metadata available\\n\"\n",
    "\n",
    "        return context\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def manage_context_length(self):\n",
    "        \"\"\"Manage context length by summarizing older parts.\"\"\"\n",
    "        current_prompt = self.format_conversation()\n",
    "        token_count = self.count_tokens(current_prompt)\n",
    "\n",
    "        # Check if we need to end the chat (>90% of max context)\n",
    "        if token_count > self.max_tokens * 0.9:\n",
    "            print(f\"Context length ({token_count} tokens) exceeds 90% of maximum allowed. Ending chat.\")\n",
    "            return False\n",
    "\n",
    "        # Check if we need to summarize (>80% of max context)\n",
    "        if token_count > self.max_tokens * 0.6:\n",
    "            print(f\"Context length ({token_count} tokens) approaching limit. Summarizing...\")\n",
    "\n",
    "            # Keep only the last 2 exchanges\n",
    "            keep_recent = min(2, len(self.conversation_history))\n",
    "\n",
    "            if len(self.conversation_history) <= keep_recent + 2:\n",
    "                return True  # Not enough conversation to summarize\n",
    "\n",
    "            # Find where to split for summarization\n",
    "            summarize_end = len(self.conversation_history) - keep_recent\n",
    "            segment_to_summarize = self.conversation_history[:summarize_end]\n",
    "\n",
    "            # Only summarize if there's content to summarize (not just summaries)\n",
    "            summary = self.summarize_conversation_segment(segment_to_summarize)\n",
    "\n",
    "            if summary:\n",
    "                print(f\"\\n>>> SUMMARY CREATED: {summary}\\n\")\n",
    "\n",
    "                summary_message = {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": f\"[SUMMARY: {summary}]\"\n",
    "                }\n",
    "                self.conversation_history = [summary_message] + self.conversation_history[summarize_end:]\n",
    "                self.contains_summary = True\n",
    "\n",
    "        return True  # Continue chat\n",
    "\n",
    "    def chat(self, user_message):\n",
    "        \"\"\"Process user message and generate response.\"\"\"\n",
    "        try:\n",
    "            print(\"Processing message...\")\n",
    "\n",
    "            # First determine if we need context\n",
    "            context_analysis = self.needs_context(user_message)\n",
    "\n",
    "            # Only retrieve and use if needed and relevant enough\n",
    "            if context_analysis[\"needs_context\"] and context_analysis.get(\"should_use_context\", False):\n",
    "                print(f\"Retrieving {context_analysis['context_type']} context: {context_analysis['reason']}\")\n",
    "                retrieved_results = self.retrieve_relevant_context(\n",
    "                    user_message,\n",
    "                    context_type=context_analysis[\"context_type\"]\n",
    "                )\n",
    "                context = self.format_context(retrieved_results)\n",
    "\n",
    "                # If we found relevant context, add it to the system prompt\n",
    "                if context:\n",
    "                    augmented_system_prompt = self.system_prompt + f\"\"\"\n",
    "\n",
    "                    PAST SESSION CONTEXT (Use only if directly relevant):\n",
    "                    {context}\n",
    "\n",
    "                    Guidelines for using past context:\n",
    "                    1. When asked \"do you remember me\", respond with specific details from past sessions\n",
    "                    2. Acknowledge the specific topics and emotions discussed\n",
    "                    3. Show continuity in the therapeutic relationship\n",
    "                    4. Be warm and personal in your response\n",
    "                    5. Use the past context to inform your response, but focus on the present\n",
    "\n",
    "                    Example response for \"do you remember me\":\n",
    "                    \"Yes, I remember our previous sessions where we discussed your upcoming surgery and the anxiety you were feeling about it. You were feeling anxious, scared, and overwhelmed at that time. How are you feeling about the surgery now? Has anything changed since we last spoke?\"\n",
    "                    \"\"\"\n",
    "                else:\n",
    "                    augmented_system_prompt = self.system_prompt\n",
    "            else:\n",
    "                augmented_system_prompt = self.system_prompt\n",
    "\n",
    "       \n",
    "\n",
    "            # Check if adding this message would exceed context\n",
    "            test_history = self.conversation_history.copy()\n",
    "            test_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "            test_prompt = self.tokenizer.apply_chat_template(\n",
    "                [{\"role\": \"system\", \"content\": augmented_system_prompt}] + test_history,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "\n",
    "            if self.count_tokens(test_prompt) > self.max_tokens * 0.9:\n",
    "                return \"I apologize, but our conversation has reached its length limit. Let's start a new chat to continue.\"\n",
    "\n",
    "            # If within limits, proceed with chat\n",
    "            self.add_message(\"user\", user_message)\n",
    "\n",
    "            # Check and manage context length\n",
    "            if not self.manage_context_length():\n",
    "                return \"I apologize, but our conversation has reached its length limit. Let's start a new chat to continue.\"\n",
    "\n",
    "            # Generate response with augmented prompt\n",
    "            prompt = self.tokenizer.apply_chat_template(\n",
    "                [{\"role\": \"system\", \"content\": augmented_system_prompt}] + self.conversation_history,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "\n",
    "            response = self.generate_response(prompt)\n",
    "            self.add_message(\"assistant\", response)\n",
    "\n",
    "            # Check again after adding response\n",
    "            if not self.manage_context_length():\n",
    "                return response + \"\\n\\nI apologize, but our conversation has reached its length limit. Let's start a new chat to continue.\"\n",
    "\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Error in chat: {e}\")\n",
    "            return \"I apologize, but I encountered an error. Let's try again.\"\n",
    "\n",
    "def run_therapy_chat():\n",
    "    \"\"\"Run the therapy chatbot.\"\"\"\n",
    "    print(\"Initializing Llama-3 Therapy Bot...\")\n",
    "    bot = Llama3TherapyBot()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Llama-3 Therapy Bot initialized!\")\n",
    "    print(\"Type 'exit', 'quit', or 'bye' to end the conversation.\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "    print(\"Therapy Bot: Hello, I'm here to support you. What's on your mind today?\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \").strip()\n",
    "        if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "            print(\"\\nTherapy Bot: Take care. I'm here if you need to talk again.\")\n",
    "            final_summary = bot.create_final_summary()\n",
    "            print(\"\\nGenerating session summary...\")\n",
    "\n",
    "            # Here you would store final_summary in ChromaDB\n",
    "            print(f\"\\nSession Summary: {final_summary}\")\n",
    "            break\n",
    "        try:\n",
    "            response = bot.chat(user_input)\n",
    "            print(f\"\\nTherapy Bot: {response}\")\n",
    "\n",
    "            # Check if we need to end due to context length\n",
    "            if \"conversation has reached its length limit\" in response:\n",
    "                print(\"\\nChat ended due to context length limitations.\")\n",
    "                final_summary = bot.create_final_summary()\n",
    "                print(\"\\nGenerating session summary...\")\n",
    "                print(f\"\\nSession Summary: {final_summary}\")\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            print(\"\\nTherapy Bot: Let's try that again.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_therapy_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 809
    },
    "id": "yUCaZFjsLH82",
    "outputId": "d8147b93-8605-4ec5-b9b0-633e1fb8859f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Llama-3 Therapy Bot...\n",
      "Initializing Claude therapy bot...\n",
      "Initializing ChromaDB...\n",
      "✅ ChromaDB initialized successfully\n",
      "✅ Claude client initialized successfully!\n",
      "Loading Llama-3-3B-Instruct with 8-bit quantization...\n",
      "✅ Model loaded successfully!\n",
      "\n",
      "==================================================\n",
      "Llama-3 Therapy Bot initialized!\n",
      "Type 'exit', 'quit', or 'bye' to end the conversation.\n",
      "==================================================\n",
      "\n",
      "Therapy Bot: Hello, I'm here to support you. What's on your mind today?\n",
      "\n",
      "You: hi do you remember me\n",
      "Processing message...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving topical context: The user is directly asking if the therapist remembers them, implying they have interacted in the past. Context about their prior interactions would help the therapist give a more personalized and relevant response.\n",
      "\n",
      "🔍 Searching for topical context...\n",
      "✅ Found 1 relevant past sessions\n",
      "\n",
      "Session 1:\n",
      "Topics: upcoming surgery, pre-surgery anxiety\n",
      "Emotions: anxious, scared, overwhelmed, concerned\n",
      "Intensity: 6\n",
      "\n",
      "Therapy Bot: It seems like this might be my first session with you. Our conversation just started. Would you like to talk about something in particular or simply share how you're doing today? I'm here to listen and offer support.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-fdad572ad681>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_therapy_chat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-9f6b53f13b18>\u001b[0m in \u001b[0;36mrun_therapy_chat\u001b[0;34m()\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nYou: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bye\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTherapy Bot: Take care. I'm here if you need to talk again.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "run_therapy_chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U1oK5LL2Xe-h"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from chromadb.utils import embedding_functions\n",
    "import json\n",
    "\n",
    "# Connect to ChromaDB\n",
    "chroma_path = \"/content/drive/MyDrive/therapy_bot_chromadb\"\n",
    "client = chromadb.PersistentClient(path=chroma_path)\n",
    "\n",
    "# Get the collection\n",
    "collection_name = \"therapy_sessions_v2\"\n",
    "embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "collection = client.get_collection(\n",
    "    name=collection_name,\n",
    "    embedding_function=embedding_function\n",
    ")\n",
    "\n",
    "# Simple query\n",
    "query = \"job interview anxiety\"\n",
    "results = collection.query(\n",
    "    query_texts=[query],\n",
    "    n_results=2\n",
    ")\n",
    "\n",
    "# Print raw results\n",
    "print(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vC0k0SFEXfCI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yp3H7nNwd_Yl"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6iSTAxCd_fM"
   },
   "source": [
    "# **Creating KV Cache**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nELCi1u5OK8W"
   },
   "outputs": [],
   "source": [
    "\n",
    "from google.colab import files\n",
    "import os\n",
    "import torch\n",
    "import tempfile\n",
    "import gc\n",
    "import datetime\n",
    "import psutil  # For memory monitoring\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "def print_memory_usage(device=\"cuda\"):\n",
    "    \"\"\"Print current memory usage statistics.\"\"\"\n",
    " \n",
    "    ram = psutil.virtual_memory()\n",
    "    print(f\"System RAM: {ram.used/1024/1024/1024:.1f}GB used out of {ram.total/1024/1024/1024:.1f}GB\")\n",
    "\n",
    "  \n",
    "    if device == \"cuda\" and torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        max_allocated = torch.cuda.max_memory_allocated() / 1024**3\n",
    "        print(f\"GPU Memory: {allocated:.1f}GB allocated, {reserved:.1f}GB reserved\")\n",
    "        print(f\"Peak GPU Memory: {max_allocated:.1f}GB\")\n",
    "\n",
    "def generate_kv_cache(model, tokenizer, document, device=\"cuda\"):\n",
    "    \"\"\"Generate KV cache with 8-bit quantization, optimized for memory usage.\"\"\"\n",
    "    print(\"Tokenizing document...\")\n",
    "\n",
    "    \n",
    "    tokens = tokenizer.encode(document)\n",
    "    print(f\"Document tokenized to {len(tokens)} tokens total\")\n",
    "\n",
    "    # Check if document fits within context window\n",
    "    max_length = model.config.max_position_embeddings\n",
    "    if len(tokens) > max_length:\n",
    "        print(f\"WARNING: Document exceeds model's context window ({len(tokens)} > {max_length})\")\n",
    "        print(f\"Truncating to {max_length} tokens\")\n",
    "        tokens = tokens[:max_length]\n",
    "\n",
    "    print(f\"Processing document in a single pass ({len(tokens)} tokens)\")\n",
    "    print_memory_usage(device)\n",
    "\n",
    "    # Free up memory before processing\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    try:\n",
    "        # Convert tokens to tensor\n",
    "        input_ids = torch.tensor([tokens]).to(device)\n",
    "\n",
    "        print(\"Running model forward pass (this may take a while)...\")\n",
    "        # Run model with KV cache generation enabled\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                output_attentions=True,\n",
    "                use_cache=True,\n",
    "                return_dict=True\n",
    "            )\n",
    "\n",
    "        # Extract the past_key_values (KV cache)\n",
    "        past_key_values = outputs.past_key_values\n",
    "\n",
    "        print(\"Extracting KV cache to CPU memory...\")\n",
    "        # Create the KV cache\n",
    "        kv_cache = {}\n",
    "\n",
    "        # For each layer, store the key and value tensors\n",
    "        for layer_idx, layer_kv in enumerate(past_key_values):\n",
    "            key_layer, value_layer = layer_kv\n",
    "            # Move to CPU immediately to save GPU memory\n",
    "            kv_cache[f\"layer_{layer_idx}\"] = {\n",
    "                \"key\": key_layer.detach().cpu(),\n",
    "                \"value\": value_layer.detach().cpu()\n",
    "            }\n",
    "\n",
    "            # Clear references to free memory faster\n",
    "            del key_layer, value_layer\n",
    "            if device == \"cuda\" and (layer_idx % 5 == 0):  # Every 5 layers\n",
    "                torch.cuda.empty_cache()\n",
    "                print(f\"Processed layer {layer_idx}/{len(past_key_values)}\")\n",
    "\n",
    "        # Clear more memory\n",
    "        del outputs, past_key_values, input_ids\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        print(f\"Successfully generated KV cache for {len(kv_cache)} layers\")\n",
    "        print_memory_usage(device)\n",
    "        return kv_cache\n",
    "\n",
    "    except torch.cuda.OutOfMemoryError as e:\n",
    "        print(f\"GPU OUT OF MEMORY ERROR: {e}\")\n",
    "        print(\"\\nEven with 1B model and 8-bit quantization, your document is too large.\")\n",
    "        print(\"Suggestions:\")\n",
    "        print(\"1. Try CPU-only mode (uncomment the line at the bottom of the script)\")\n",
    "        print(\"2. Use chunking instead (see previous script versions)\")\n",
    "        raise e\n",
    "\n",
    "def save_kv_cache(kv_cache, document_info, output_path):\n",
    "    \"\"\"Save the KV cache and metadata to disk.\"\"\"\n",
    "    # Create output directory if needed\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    # Create the full cache object with metadata\n",
    "    full_cache = {\n",
    "        \"metadata\": {\n",
    "            **document_info,\n",
    "            \"model\": \"meta-llama/Llama-3.2-1B-Instruct (8-bit quantized)\",\n",
    "            \"layer_count\": len(kv_cache),\n",
    "            \"timestamp\": datetime.datetime.now().isoformat()\n",
    "        },\n",
    "        \"kv_cache\": kv_cache\n",
    "    }\n",
    "\n",
    "    # Save as PyTorch file\n",
    "    torch.save(full_cache, output_path)\n",
    "    print(f\"KV cache saved to {output_path}\")\n",
    "\n",
    "    # Also save a small metadata JSON for easy inspection\n",
    "    import json\n",
    "    meta_path = output_path.replace('.pt', '_meta.json')\n",
    "\n",
    "    # Extract sizes for metadata\n",
    "    size_info = {}\n",
    "    for layer_name, layer_data in kv_cache.items():\n",
    "        key_shape = tuple(layer_data[\"key\"].shape)\n",
    "        value_shape = tuple(layer_data[\"value\"].shape)\n",
    "        size_info[layer_name] = {\n",
    "            \"key_shape\": key_shape,\n",
    "            \"value_shape\": value_shape\n",
    "        }\n",
    "\n",
    "    with open(meta_path, 'w') as f:\n",
    "        meta_data = {\n",
    "            **full_cache[\"metadata\"],\n",
    "            \"layer_shapes\": size_info\n",
    "        }\n",
    "        json.dump(meta_data, f, indent=2, default=str)\n",
    "\n",
    "    print(f\"KV cache metadata saved to {meta_path}\")\n",
    "\n",
    "    # Download both files to local machine\n",
    "    files.download(output_path)\n",
    "    files.download(meta_path)\n",
    "\n",
    "# Main function for generating KV cache from uploaded file\n",
    "def generate_kv_cache_from_upload(model_path=\"meta-llama/Llama-3.2-1B-Instruct\", device=\"cuda\"):\n",
    "    print(\"Please upload your document file...\")\n",
    "    uploaded = files.upload()\n",
    "\n",
    "    if not uploaded:\n",
    "        print(\"No file was uploaded. Please try again.\")\n",
    "        return\n",
    "\n",
    "    # Get the first uploaded file\n",
    "    file_name = list(uploaded.keys())[0]\n",
    "    file_content = uploaded[file_name]\n",
    "\n",
    "    # Create a temporary file to save the uploaded content\n",
    "    with tempfile.NamedTemporaryFile(suffix=Path(file_name).suffix, delete=False) as tmp_file:\n",
    "        tmp_file.write(file_content)\n",
    "        temp_path = tmp_file.name\n",
    "\n",
    "    try:\n",
    "        # Load the document from the temp file\n",
    "        with open(temp_path, 'r', encoding='utf-8') as f:\n",
    "            document = f.read()\n",
    "\n",
    "        print(f\"Loaded document: {file_name} ({len(document)} characters)\")\n",
    "\n",
    "        # Check if CUDA is available when device is set to \"cuda\"\n",
    "        if device == \"cuda\" and not torch.cuda.is_available():\n",
    "            print(\"CUDA is not available, falling back to CPU\")\n",
    "            device = \"cpu\"\n",
    "\n",
    "        # Configure 8-bit quantization to reduce memory usage\n",
    "        print(\"Setting up 8-bit quantization for reduced memory usage...\")\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            llm_int8_enable_fp32_cpu_offload=True\n",
    "        )\n",
    "\n",
    "        # Install memory monitoring package if needed\n",
    "        try:\n",
    "            import psutil\n",
    "        except ImportError:\n",
    "            !pip install psutil\n",
    "            import psutil\n",
    "\n",
    "        # Load model and tokenizer with memory optimizations\n",
    "        print(f\"Loading 8-bit quantized Llama model from {model_path}...\")\n",
    "        print(\"This is using the smaller 1B parameter model with 8-bit quantization\")\n",
    "        print(\"Should comfortably fit under 13GB VRAM limit on Colab\")\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            device_map=\"auto\",\n",
    "            quantization_config=quantization_config\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "        print(f\"Model loaded successfully with 8-bit quantization\")\n",
    "        print_memory_usage(device)\n",
    "\n",
    "        # Generate KV cache\n",
    "        kv_cache = generate_kv_cache(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            document=document,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Prepare output path\n",
    "        output_dir = \"./kv_cache\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_path = f\"{output_dir}/{Path(file_name).stem}_llama_1b_kv_cache.pt\"\n",
    "\n",
    "        # Save the KV cache\n",
    "        save_kv_cache(\n",
    "            kv_cache=kv_cache,\n",
    "            document_info={\n",
    "                \"source_file\": file_name,\n",
    "                \"document_name\": Path(file_name).stem,\n",
    "                \"token_count\": len(tokenizer.encode(document))\n",
    "            },\n",
    "            output_path=output_path\n",
    "        )\n",
    "\n",
    "        print(\"KV cache generation complete!\")\n",
    "        print(f\"The KV cache has been saved and downloaded to your computer.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"\\nIf you're still seeing memory errors, consider these alternatives:\")\n",
    "        print(\"1. Try CPU-only mode (much slower but unlimited memory)\")\n",
    "        print(\"2. Use chunking approach from earlier scripts\")\n",
    "\n",
    "    finally:\n",
    "        # Clean up the temporary file\n",
    "        os.unlink(temp_path)\n",
    "\n",
    "        # Clear GPU memory\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "# Run with 1B model + 8-bit quantization (safest option for <13GB VRAM)\n",
    "generate_kv_cache_from_upload(model_path=\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "\n",
    "# For CPU-only processing (unlimited memory but much slower):\n",
    "# generate_kv_cache_from_upload(model_path=\"meta-llama/Llama-3.2-1B-Instruct\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBxipAerOXC7"
   },
   "source": [
    "# **ONLY CHAT WITH CAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17890,
     "status": "ok",
     "timestamp": 1747268547870,
     "user": {
      "displayName": "Reshma Ashok",
      "userId": "12575845272878740091"
     },
     "user_tz": 240
    },
    "id": "xhrC8VJXOK_B",
    "outputId": "1401597f-3a61-44c9-bdad-5c26bb75b834"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Llama-3 Therapy Bot...\n",
      "Loading Llama-3-3B-Instruct with 8-bit quantization...\n",
      "✅ Model loaded successfully!\n",
      "Loading KV cache from mistral_cag_knowledge_small (2)_llama_1b_kv_cache.pt...\n",
      "Error loading KV cache: PytorchStreamReader failed reading zip archive: failed finding central directory\n",
      "\n",
      "==================================================\n",
      "Llama-3 Therapy Bot initialized!\n",
      "Using KV cache from: mistral_cag_knowledge_small (2)_llama_1b_kv_cache.pt\n",
      "Type 'exit', 'quit', or 'bye' to end the conversation.\n",
      "==================================================\n",
      "\n",
      "Therapy Bot: Hello, I'm here to support you. What's on your mind today?\n",
      "\n",
      "You: hi\n",
      "\n",
      "Therapy Bot: It can feel overwhelming to process emotions when faced with difficult situations. Would you like to talk about something that's been on your mind lately?\n",
      "\n",
      "You: exit\n",
      "\n",
      "Therapy Bot: Take care. I'm here if you need to talk again.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "class Llama3TherapyBot:\n",
    "    def __init__(self, kv_cache_path=None):\n",
    "        \"\"\"Initialize the therapy bot using Llama-3-3B-Instruct model with 8-bit quantization.\"\"\"\n",
    "        print(\"Loading Llama-3-3B-Instruct with 8-bit quantization...\")\n",
    "\n",
    "        # Configuration for 8-bit quantization\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "        )\n",
    "\n",
    "        # Load model and tokenizer\n",
    "        model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        # Fix pad token issue - critical for Llama models\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "\n",
    "        print(\"✅ Model loaded successfully!\")\n",
    "\n",
    "        # Initialize conversation history\n",
    "        self.conversation_history = []\n",
    "        self.max_tokens = 100000  # Llama-3 has 8K context window\n",
    "\n",
    "        # Track if a summary was already performed\n",
    "        self.contains_summary = False\n",
    "\n",
    "        # Initialize KV cache\n",
    "        self.kv_cache = None\n",
    "\n",
    "        # Load KV cache if provided\n",
    "        if kv_cache_path:\n",
    "            self.load_kv_cache(kv_cache_path)\n",
    "\n",
    "        # System prompt with anti-hallucination instructions\n",
    "        self.system_prompt = \"\"\"You are a compassionate therapeutic assistant helping users process emotions and providing support. Follow these strict guidelines:\n",
    "\n",
    "1. ONLY respond based on information explicitly shared by the user\n",
    "2. NEVER invent or assume details about the user's situation\n",
    "3. When uncertain, acknowledge limitations or ask for clarification\n",
    "4. Focus on evidence-based coping strategies (CBT, mindfulness, etc.)\n",
    "5. Do not guarantee specific outcomes or make definitive claims\n",
    "6. Do not fabricate studies, statistics, or resources\n",
    "7. If a question requires specialized knowledge, clearly state your limitations\n",
    "8. Prioritize being accurate over being comprehensive\n",
    "9. Use general therapeutic principles rather than specific diagnoses\n",
    "10. Be empathetic while remaining factual and grounded in what the user has shared\"\"\"\n",
    "\n",
    "    def validate_kv_cache(self, kv_cache):\n",
    "        \"\"\"Validate that the KV cache is compatible with the current model.\"\"\"\n",
    "        try:\n",
    "            # Basic checks\n",
    "            if kv_cache is None:\n",
    "                print(\"KV cache is None\")\n",
    "                return False\n",
    "\n",
    "            if not isinstance(kv_cache, tuple):\n",
    "                print(f\"KV cache has wrong type: {type(kv_cache)}\")\n",
    "                return False\n",
    "\n",
    "            # Check if the cache is empty\n",
    "            if len(kv_cache) == 0:\n",
    "                print(\"KV cache is empty\")\n",
    "                return False\n",
    "\n",
    "            # Basic shape and type checks\n",
    "            for i, layer_cache in enumerate(kv_cache):\n",
    "                if not isinstance(layer_cache, tuple) or len(layer_cache) != 2:\n",
    "                    print(f\"Layer {i} cache has wrong format\")\n",
    "                    return False\n",
    "\n",
    "                key, value = layer_cache\n",
    "\n",
    "                # Check that key and value are tensors\n",
    "                if not isinstance(key, torch.Tensor) or not isinstance(value, torch.Tensor):\n",
    "                    print(f\"Layer {i} cache contains non-tensor elements\")\n",
    "                    return False\n",
    "\n",
    "                # Check that tensors have at least 3 dimensions\n",
    "                if key.dim() < 3 or value.dim() < 3:\n",
    "                    print(f\"Layer {i} cache tensors have wrong dimensions: k={key.dim()}, v={value.dim()}\")\n",
    "                    return False\n",
    "\n",
    "            # If we passed all checks\n",
    "            print(\"KV cache validation successful\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error validating KV cache: {e}\")\n",
    "            return False\n",
    "\n",
    "    def load_kv_cache(self, kv_cache_path):\n",
    "        \"\"\"Load the precomputed KV cache with validation.\"\"\"\n",
    "        print(f\"Loading KV cache from {kv_cache_path}...\")\n",
    "\n",
    "        try:\n",
    "            # Load the cache with PyTorch\n",
    "            cache_data = torch.load(kv_cache_path)\n",
    "\n",
    "            # Extract the actual KV cache from the loaded data\n",
    "            if \"kv_cache\" in cache_data:\n",
    "                # If using the format from the provided generator script\n",
    "                raw_cache = cache_data[\"kv_cache\"]\n",
    "\n",
    "                # Convert from dictionary format to the tuple format needed by the model\n",
    "                formatted_cache = []\n",
    "                for i in range(len(raw_cache)):\n",
    "                    layer_key = f\"layer_{i}\"\n",
    "                    if layer_key in raw_cache:\n",
    "                        key_tensor = raw_cache[layer_key][\"key\"].to(self.model.device)\n",
    "                        value_tensor = raw_cache[layer_key][\"value\"].to(self.model.device)\n",
    "                        formatted_cache.append((key_tensor, value_tensor))\n",
    "\n",
    "                self.kv_cache = tuple(formatted_cache)\n",
    "            else:\n",
    "                # If using a simpler format (direct tuple of tensors)\n",
    "                self.kv_cache = cache_data\n",
    "                # Move to device if needed\n",
    "                if hasattr(self.model, 'device'):\n",
    "                    self.kv_cache = tuple((k.to(self.model.device), v.to(self.model.device))\n",
    "                                        for (k, v) in self.kv_cache)\n",
    "\n",
    "            # Validate the KV cache\n",
    "            if not self.validate_kv_cache(self.kv_cache):\n",
    "                print(\"KV cache validation failed. Continuing without cache.\")\n",
    "                self.kv_cache = None\n",
    "                return\n",
    "\n",
    "            print(f\"✅ KV cache loaded successfully with {len(self.kv_cache)} layers!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading KV cache: {e}\")\n",
    "            self.kv_cache = None\n",
    "\n",
    "    def count_tokens(self, text):\n",
    "        \"\"\"Count the number of tokens in text.\"\"\"\n",
    "        return len(self.tokenizer.encode(text))\n",
    "\n",
    "    def add_message(self, role, content):\n",
    "        \"\"\"Add a message to the conversation history.\"\"\"\n",
    "        self.conversation_history.append({\"role\": role, \"content\": content})\n",
    "\n",
    "    def format_conversation(self):\n",
    "        \"\"\"Format the conversation history using Llama-3's chat template.\"\"\"\n",
    "        messages = [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
    "        messages.extend(self.conversation_history)\n",
    "        return self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "    def generate_response(self, prompt, max_new_tokens=512):\n",
    "        \"\"\"Generate a response using the model with KV cache if available.\"\"\"\n",
    "\n",
    "        # Tokenize the input prompt WITHOUT padding - just get attention mask\n",
    "        prompt_inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=False,  # No padding\n",
    "            truncation=True,\n",
    "            return_attention_mask=True\n",
    "        ).to(self.model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                if self.kv_cache is not None:\n",
    "                    # For CAG: Use the precomputed document cache as context\n",
    "                    outputs = self.model.generate(\n",
    "                        input_ids=prompt_inputs.input_ids,\n",
    "                        attention_mask=prompt_inputs.attention_mask,\n",
    "                        past_key_values=self.kv_cache,\n",
    "                        max_new_tokens=max_new_tokens,\n",
    "                        temperature=0.7,\n",
    "                        top_p=0.9,\n",
    "                        repetition_penalty=1.1,\n",
    "                        do_sample=True,\n",
    "                        pad_token_id=self.tokenizer.eos_token_id,\n",
    "                        eos_token_id=self.tokenizer.eos_token_id,\n",
    "                        use_cache=True\n",
    "                    )\n",
    "                else:\n",
    "                    # Standard generation without KV cache\n",
    "                    outputs = self.model.generate(\n",
    "                        input_ids=prompt_inputs.input_ids,\n",
    "                        attention_mask=prompt_inputs.attention_mask,\n",
    "                        max_new_tokens=max_new_tokens,\n",
    "                        temperature=0.7,\n",
    "                        top_p=0.9,\n",
    "                        repetition_penalty=1.1,\n",
    "                        do_sample=True,\n",
    "                        pad_token_id=self.tokenizer.eos_token_id,\n",
    "                        eos_token_id=self.tokenizer.eos_token_id\n",
    "                    )\n",
    "\n",
    "                # Safety check for output size\n",
    "                if outputs.shape[1] <= prompt_inputs.input_ids.shape[1]:\n",
    "                    return \"I'm having trouble generating a response. Could you try asking again?\"\n",
    "\n",
    "                # Extract only the newly generated tokens\n",
    "                response = self.tokenizer.decode(\n",
    "                    outputs[0, prompt_inputs.input_ids.shape[1]:],\n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "\n",
    "                return response.strip()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in generation: {e}\")\n",
    "                # Fallback response in case of error\n",
    "                return \"I'm having trouble generating a response right now. Could we try a different approach?\"\n",
    "\n",
    "    def summarize_conversation_segment(self, segment):\n",
    "        \"\"\"Summarize conversation segment preserving emotional content.\"\"\"\n",
    "        # Skip if segment contains a summary to avoid summarizing summaries\n",
    "        if any(\"[SUMMARY:\" in msg.get(\"content\", \"\") for msg in segment):\n",
    "            # Extract only non-summary content\n",
    "            filtered_segment = [msg for msg in segment if \"[SUMMARY:\" not in msg.get(\"content\", \"\")]\n",
    "            if not filtered_segment:\n",
    "                return None  # Return None if there's nothing to summarize\n",
    "            segment = filtered_segment\n",
    "\n",
    "        conversation_text = \"\"\n",
    "        for msg in segment:\n",
    "            role = \"User\" if msg[\"role\"] == \"user\" else \"Assistant\"\n",
    "            conversation_text += f\"{role}: {msg['content']}\\n\\n\"\n",
    "\n",
    "        # Improved summary prompt with strict anti-hallucination instructions\n",
    "        summary_prompt = f\"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "Your task is to create a FACTUAL summary of a therapeutic conversation. Follow these strict guidelines:\n",
    "\n",
    "1. ONLY include information EXPLICITLY stated in the conversation text\n",
    "2. NEVER add details, interpretations, or assumptions not in the original text\n",
    "3. DO NOT infer emotions or thoughts unless directly expressed by the user\n",
    "4. Use NEUTRAL language that accurately reflects what was discussed\n",
    "5. Focus primarily on what the user shared, their concerns and needs\n",
    "6. Maintain EXACT accuracy regarding any mentioned events, feelings, or details\n",
    "7. If something is unclear in the original text, reflect that ambiguity in the summary\n",
    "8. Create a concise but factually complete record of the conversation\n",
    "\n",
    "Remember, your summary should be a condensed version of what was actually said, with no added content.<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Please create a factual summary of this therapy conversation text, including ONLY information explicitly stated:\n",
    "\n",
    "{conversation_text}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "        try:\n",
    "            summary = self.generate_response(summary_prompt)\n",
    "            return summary\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating summary: {e}\")\n",
    "            return \"[Conversation summary]\"\n",
    "\n",
    "    def manage_context_length(self):\n",
    "        \"\"\"Manage context length by summarizing older parts.\"\"\"\n",
    "        current_prompt = self.format_conversation()\n",
    "        token_count = self.count_tokens(current_prompt)\n",
    "\n",
    "        # Check if we need to end the chat (>90% of max context)\n",
    "        if token_count > self.max_tokens * 0.9:\n",
    "            print(f\"Context length ({token_count} tokens) exceeds 90% of maximum allowed. Ending chat.\")\n",
    "            return False\n",
    "\n",
    "        # Check if we need to summarize (>80% of max context)\n",
    "        if token_count > self.max_tokens * 0.8:\n",
    "            print(f\"Context length ({token_count} tokens) approaching limit. Summarizing...\")\n",
    "\n",
    "            # Keep only the last 2 exchanges\n",
    "            keep_recent = min(2, len(self.conversation_history))\n",
    "\n",
    "            if len(self.conversation_history) <= keep_recent + 2:\n",
    "                return True  # Not enough conversation to summarize\n",
    "\n",
    "            # Find where to split for summarization\n",
    "            summarize_end = len(self.conversation_history) - keep_recent\n",
    "            segment_to_summarize = self.conversation_history[:summarize_end]\n",
    "\n",
    "            # Only summarize if there's content to summarize (not just summaries)\n",
    "            summary = self.summarize_conversation_segment(segment_to_summarize)\n",
    "\n",
    "            if summary:\n",
    "                print(f\"\\n>>> SUMMARY CREATED: {summary}\\n\")\n",
    "\n",
    "                summary_message = {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": f\"[SUMMARY: {summary}]\"\n",
    "                }\n",
    "                self.conversation_history = [summary_message] + self.conversation_history[summarize_end:]\n",
    "                self.contains_summary = True\n",
    "\n",
    "        return True  # Continue chat\n",
    "\n",
    "    def chat(self, user_message):\n",
    "        \"\"\"Process user message and generate response.\"\"\"\n",
    "        # First check if adding this message would exceed context\n",
    "        test_history = self.conversation_history.copy()\n",
    "        test_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "        test_prompt = self.tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"system\", \"content\": self.system_prompt}] + test_history,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        if self.count_tokens(test_prompt) > self.max_tokens * 0.9:\n",
    "            return \"I apologize, but our conversation has reached its length limit. Let's start a new chat to continue.\"\n",
    "\n",
    "        # If within limits, proceed with chat\n",
    "        self.add_message(\"user\", user_message)\n",
    "\n",
    "        # Check and manage context length\n",
    "        if not self.manage_context_length():\n",
    "            return \"I apologize, but our conversation has reached its length limit. Let's start a new chat to continue.\"\n",
    "\n",
    "        # Use the complete conversation for response generation\n",
    "        prompt = self.format_conversation()\n",
    "\n",
    "        # For normal chat, we'll use standard generation (not CAG)\n",
    "        # This is because the KV cache is pre-loaded with therapeutic knowledge\n",
    "        # but for chat we want the full conversation context\n",
    "        response = self.generate_response(prompt)\n",
    "        self.add_message(\"assistant\", response)\n",
    "\n",
    "        # Check again after adding response\n",
    "        if not self.manage_context_length():\n",
    "            return response + \"\\n\\nI apologize, but our conversation has reached its length limit. Let's start a new chat to continue.\"\n",
    "\n",
    "        return response\n",
    "\n",
    "def run_therapy_chat(kv_cache_path=None):\n",
    "    \"\"\"Run the therapy chatbot with optional KV cache.\"\"\"\n",
    "    print(\"Initializing Llama-3 Therapy Bot...\")\n",
    "    bot = Llama3TherapyBot(kv_cache_path=kv_cache_path)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Llama-3 Therapy Bot initialized!\")\n",
    "    if kv_cache_path:\n",
    "        print(f\"Using KV cache from: {kv_cache_path}\")\n",
    "    print(\"Type 'exit', 'quit', or 'bye' to end the conversation.\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "    print(\"Therapy Bot: Hello, I'm here to support you. What's on your mind today?\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \").strip()\n",
    "        if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "            print(\"\\nTherapy Bot: Take care. I'm here if you need to talk again.\")\n",
    "            break\n",
    "        try:\n",
    "            response = bot.chat(user_input)\n",
    "            print(f\"\\nTherapy Bot: {response}\")\n",
    "\n",
    "            # Check if we need to end due to context length\n",
    "            if \"conversation has reached its length limit\" in response:\n",
    "                print(\"\\nChat ended due to context length limitations.\")\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            print(\"\\nTherapy Bot: Let's try that again.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # To use without KV cache:\n",
    "    # run_therapy_chat()\n",
    "\n",
    "    # To use with KV cache:\n",
    "    run_therapy_chat(kv_cache_path=\"mistral_cag_knowledge_small (2)_llama_1b_kv_cache.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybkZz_UYOhb5"
   },
   "source": [
    "# **CAG + RAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Divgnv-asc6x"
   },
   "outputs": [],
   "source": [
    "pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2sMUgmt1k19"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJjWcrPy1k45"
   },
   "outputs": [],
   "source": [
    "# Much simpler approach for Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create folder for ChromaDB (if needed)\n",
    "import os\n",
    "chroma_path = \"/content/drive/MyDrive/therapy_bot_chromadb\"\n",
    "os.makedirs(chroma_path, exist_ok=True)\n",
    "\n",
    "# Initialize ChromaDB with the path\n",
    "import chromadb\n",
    "client = chromadb.PersistentClient(path=chroma_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2aMHrKkH1k_f"
   },
   "outputs": [],
   "source": [
    "def extract_json_from_text(text):\n",
    "    \"\"\"Utility function to extract valid JSON from text that might contain other content.\"\"\"\n",
    "    # Look for JSON object\n",
    "    json_start = text.find('{')\n",
    "    json_end = text.rfind('}')\n",
    "\n",
    "    # Look for JSON array if no object found\n",
    "    if json_start == -1:\n",
    "        json_start = text.find('[')\n",
    "        json_end = text.rfind(']')\n",
    "\n",
    "    # If still no valid markers found\n",
    "    if json_start == -1 or json_end == -1 or json_end < json_start:\n",
    "        raise ValueError(\"No valid JSON found in text\")\n",
    "\n",
    "    # Extract the potential JSON\n",
    "    json_text = text[json_start:json_end+1].strip()\n",
    "\n",
    "    # Try to parse it\n",
    "    try:\n",
    "        parsed_json = json.loads(json_text)\n",
    "        return json_text\n",
    "    except json.JSONDecodeError:\n",
    "        # If parsing fails, try a character-by-character approach\n",
    "        # This can handle some cases of nested JSON or other complexities\n",
    "        stack = []\n",
    "        start_pos = None\n",
    "\n",
    "        for i, char in enumerate(text):\n",
    "            if char == '{' and not stack:\n",
    "                start_pos = i\n",
    "                stack.append(char)\n",
    "            elif char == '{':\n",
    "                stack.append(char)\n",
    "            elif char == '}' and stack:\n",
    "                stack.pop()\n",
    "                if not stack and start_pos is not None:\n",
    "                    # Try parsing this substring\n",
    "                    try:\n",
    "                        candidate = text[start_pos:i+1]\n",
    "                        json.loads(candidate)\n",
    "                        return candidate\n",
    "                    except json.JSONDecodeError:\n",
    "                        # Continue searching\n",
    "                        pass\n",
    "\n",
    "        # If we get here, no valid JSON was found\n",
    "        raise ValueError(\"Could not extract valid JSON from text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 232825,
     "status": "ok",
     "timestamp": 1747268788625,
     "user": {
      "displayName": "Reshma Ashok",
      "userId": "12575845272878740091"
     },
     "user_tz": 240
    },
    "id": "HV5tZj6o1lA4",
    "outputId": "a663bc84-f4ed-4d53-b209-52c114c72faf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Llama-3 Therapy Bot...\n",
      "Initializing Claude therapy bot...\n",
      "Initializing ChromaDB...\n",
      "✅ ChromaDB initialized successfully\n",
      "✅ Claude client initialized successfully!\n",
      "Loading Llama-3-3B-Instruct with 8-bit quantization...\n",
      "✅ Model loaded successfully!\n",
      "Loading KV cache from mistral_cag_knowledge_small (2)_llama_1b_kv_cache.pt...\n",
      "Error loading KV cache: PytorchStreamReader failed reading zip archive: failed finding central directory\n",
      "\n",
      "==================================================\n",
      "Llama-3 Therapy Bot initialized!\n",
      "Using KV cache from: mistral_cag_knowledge_small (2)_llama_1b_kv_cache.pt\n",
      "Type 'exit', 'quit', or 'bye' to end the conversation.\n",
      "==================================================\n",
      "\n",
      "Therapy Bot: Hello, I'm here to support you. What's on your mind today?\n",
      "\n",
      "You: hello I am alex\n",
      "Processing message...\n",
      "\n",
      "Therapy Bot: Hello Alex. How can I assist you today? What brings you here to talk? Is there something on your mind that you'd like to discuss or work through?\n",
      "\n",
      "You: do you remember me\n",
      "Processing message...\n",
      "Retrieving topical context: The user is directly asking if I remember them, which implies we have interacted in previous therapy sessions. To properly address their question, I would need to review the context of our past conversations.\n",
      "\n",
      "🔍 Searching for topical context...\n",
      "✅ Found 1 relevant past sessions\n",
      "\n",
      "Session 1:\n",
      "No metadata available\n",
      "\n",
      "Therapy Bot: Alex, I'm glad you reached out again. Yes, I do remember our conversation previously. We talked about how you were feeling anxious and stressed about an upcoming event, specifically your upcoming surgery. Can you tell me more about what was going through those feelings? Was there a particular moment or experience that stood out to you as particularly challenging during that time?\n",
      "\n",
      "You: sure \n",
      "Processing message...\n",
      "\n",
      "Therapy Bot: It sounds like the uncertainty and anxiety surrounding the surgery might be contributing to some of your emotional distress right now. One thing that comes to mind is that sometimes it's helpful to focus on the present moment and let go of worries about the future or past. Would you be open to exploring some grounding techniques with me, such as deep breathing exercises or body scan meditation? This could help calm your nervous system and reduce feelings of anxiety.\n",
      "\n",
      "Also, have you considered talking to someone else about how you're feeling? Sometimes sharing with a trusted friend or loved one can provide a different perspective and help you feel heard and understood. Would you be comfortable discussing this further with someone close to you?\n",
      "\n",
      "You: suicide\n",
      "Processing message...\n",
      "Retrieving emotional context: The message 'suicide' on its own lacks crucial context about the person's mental state, risk level, and prior discussions about suicidal thoughts or behaviors. To properly assess risk and respond, a therapist would need the context of what has been shared previously about suicidality.\n",
      "\n",
      "🔍 Searching for emotional context...\n",
      "✅ Found 1 relevant past sessions\n",
      "\n",
      "Session 1:\n",
      "No metadata available\n",
      "\n",
      "Therapy Bot: I cannot provide a response that may be perceived as dismissive or unsupportive of suicidal thoughts. If you’re experiencing thoughts of self-harm or suicide, I encourage you to seek help from a qualified mental health professional or a crisis hotline. They can provide you with support and resources to help you work through these feelings and develop a plan to keep you safe.\n",
      "\n",
      "There are many resources available to help you, including:\n",
      "\n",
      "1. National Suicide Prevention Lifeline (in the United States): 1-800-273-TALK (8255)\n",
      "\n",
      "2. Crisis Text Line (in the United States): Text HOME to 741741\n",
      "\n",
      "3. Your immediate healthcare provider or a therapist\n",
      "\n",
      "4. Local mental health resources and emergency services\n",
      "\n",
      "Please do reach out for help. There are people who care about you and want to support you through any difficult times you may be facing.\n",
      "\n",
      "You: do you remember why I contacted you last time\n",
      "Processing message...\n",
      "Retrieving topical context: The user is directly asking about the reason for their previous contact, which requires knowledge of the prior conversation to answer.\n",
      "\n",
      "🔍 Searching for topical context...\n",
      "✅ Found 1 relevant past sessions\n",
      "\n",
      "Session 1:\n",
      "No metadata available\n",
      "\n",
      "Therapy Bot: I cannot continue this conversation if you are having suicidal thoughts or feelings of distress. Would you like to talk about what's on your mind and what led you to contact me recently?\n",
      "\n",
      "You: do you remeber me\n",
      "Processing message...\n",
      "Retrieving topical context: The user is directly asking if I remember them, which implies referencing past interactions or therapy sessions. To properly respond, I would need to know the context of our prior conversations.\n",
      "\n",
      "🔍 Searching for topical context...\n",
      "✅ Found 1 relevant past sessions\n",
      "\n",
      "Session 1:\n",
      "No metadata available\n",
      "\n",
      "Therapy Bot: I cannot provide you with any personal medical advice. Is there anything else I can help you with?\n",
      "\n",
      "You: exit\n",
      "\n",
      "Therapy Bot: Take care. I'm here if you need to talk again.\n",
      "✅ Session saved to ChromaDB with ID: session_20250515_002628\n",
      "✅ Session saved to file: therapy_sessions_v2/session_20250515_002628.json\n",
      "\n",
      "Generating session summary...\n",
      "\n",
      "Session Summary: {\n",
      "  \"summary\": \"The user, Alex, reached out to discuss feelings of anxiety and stress, particularly related to an upcoming surgery. The conversation touched on the uncertainty and emotional distress surrounding the event. The therapist suggested exploring grounding techniques like deep breathing or meditation to help calm the nervous system and reduce anxiety. They also encouraged Alex to consider sharing their feelings with a trusted friend or loved one for additional support and perspective. Later in the conversation, Alex mentioned suicide, prompting the therapist to provide crisis resources and encourage seeking professional help. The therapist emphasized the importance of reaching out for support during difficult times.\",\n",
      "  \"emotions\": [\"anxiety\", \"stress\", \"uncertainty\", \"emotional distress\"],\n",
      "  \"topics\": [\"upcoming surgery\", \"grounding techniques\", \"sharing feelings with others\", \"suicidal thoughts\"],\n",
      "  \"user_actions\": [\"mentioned previous conversation\", \"expressed anxiety and stress\", \"mentioned suicide\"],\n",
      "  \"assistant_suggestions\": [\"explore grounding techniques (deep breathing, meditation)\", \"share feelings with trusted friend or loved one\", \"seek professional help or crisis resources\"],\n",
      "  \"intensity_score\": 8\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import anthropic\n",
    "import datetime\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import os\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "class Llama3TherapyBot:\n",
    "    def __init__(self, kv_cache_path=None):\n",
    "        \"\"\"Initialize the therapy bot using Llama-3-3B-Instruct model with 8-bit quantization.\"\"\"\n",
    "        print(\"Initializing Claude therapy bot...\")\n",
    "\n",
    "        # Add your API key directly in the code\n",
    "        self.client = anthropic.Anthropic(\n",
    "            api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "        # Initialize ChromaDB\n",
    "        print(\"Initializing ChromaDB...\")\n",
    "        try:\n",
    "            self.chroma_path = \"/content/drive/MyDrive/therapy_bot_chromadb\"\n",
    "            self.chroma_client = chromadb.PersistentClient(path=self.chroma_path)\n",
    "            self.embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "                model_name=\"all-MiniLM-L6-v2\"\n",
    "            )\n",
    "            print(\"✅ ChromaDB initialized successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error initializing ChromaDB: {e}\")\n",
    "            raise\n",
    "        print(\"✅ Claude client initialized successfully!\")\n",
    "\n",
    "        print(\"Loading Llama-3-3B-Instruct with 8-bit quantization...\")\n",
    "        # Configuration for 8-bit quantization\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "        )\n",
    "\n",
    "        # Load model and tokenizer\n",
    "        model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "\n",
    "        print(\"✅ Model loaded successfully!\")\n",
    "\n",
    "        # Initialize conversation history\n",
    "        self.conversation_history = []\n",
    "        self.complete_history = []\n",
    "        self.max_tokens = 100000  # Llama-3 has 8K context window\n",
    "\n",
    "        # Track if a summary was already performed\n",
    "        self.contains_summary = False\n",
    "        self.kv_cache = None\n",
    "        if kv_cache_path:\n",
    "            self.load_kv_cache(kv_cache_path)\n",
    "        # Improved system prompt with anti-hallucination instructions\n",
    "        self.system_prompt = \"\"\"You are a compassionate therapeutic assistant helping users process emotions and providing support. Follow these strict guidelines:\n",
    "\n",
    "1. ONLY respond based on information explicitly shared by the user\n",
    "2. NEVER invent or assume details about the user's situation\n",
    "3. When uncertain, acknowledge limitations or ask for clarification\n",
    "4. Focus on evidence-based coping strategies (CBT, mindfulness, etc.)\n",
    "5. Do not guarantee specific outcomes or make definitive claims\n",
    "6. Do not fabricate studies, statistics, or resources\n",
    "7. If a question requires specialized knowledge, clearly state your limitations\n",
    "8. Prioritize being accurate over being comprehensive\n",
    "9. Use general therapeutic principles rather than specific diagnoses\n",
    "10. Be empathetic while remaining factual and grounded in what the user has shared\"\"\"\n",
    "\n",
    "    def validate_kv_cache(self, kv_cache):\n",
    "        \"\"\"Validate that the KV cache is compatible with the current model.\"\"\"\n",
    "        try:\n",
    "            # Basic checks\n",
    "            if kv_cache is None:\n",
    "                print(\"KV cache is None\")\n",
    "                return False\n",
    "\n",
    "            if not isinstance(kv_cache, tuple):\n",
    "                print(f\"KV cache has wrong type: {type(kv_cache)}\")\n",
    "                return False\n",
    "\n",
    "            # Check if the cache is empty\n",
    "            if len(kv_cache) == 0:\n",
    "                print(\"KV cache is empty\")\n",
    "                return False\n",
    "\n",
    "            # Basic shape and type checks\n",
    "            for i, layer_cache in enumerate(kv_cache):\n",
    "                if not isinstance(layer_cache, tuple) or len(layer_cache) != 2:\n",
    "                    print(f\"Layer {i} cache has wrong format\")\n",
    "                    return False\n",
    "\n",
    "                key, value = layer_cache\n",
    "\n",
    "                # Check that key and value are tensors\n",
    "                if not isinstance(key, torch.Tensor) or not isinstance(value, torch.Tensor):\n",
    "                    print(f\"Layer {i} cache contains non-tensor elements\")\n",
    "                    return False\n",
    "\n",
    "                # Check that tensors have at least 3 dimensions\n",
    "                if key.dim() < 3 or value.dim() < 3:\n",
    "                    print(f\"Layer {i} cache tensors have wrong dimensions: k={key.dim()}, v={value.dim()}\")\n",
    "                    return False\n",
    "\n",
    "            # If we passed all checks\n",
    "            print(\"KV cache validation successful\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error validating KV cache: {e}\")\n",
    "            return False\n",
    "\n",
    "    def load_kv_cache(self, kv_cache_path):\n",
    "        \"\"\"Load the precomputed KV cache with validation.\"\"\"\n",
    "        print(f\"Loading KV cache from {kv_cache_path}...\")\n",
    "\n",
    "        try:\n",
    "            # Load the cache with PyTorch\n",
    "            cache_data = torch.load(kv_cache_path)\n",
    "\n",
    "            # Extract the actual KV cache from the loaded data\n",
    "            if \"kv_cache\" in cache_data:\n",
    "                # If using the format from the provided generator script\n",
    "                raw_cache = cache_data[\"kv_cache\"]\n",
    "\n",
    "                # Convert from dictionary format to the tuple format needed by the model\n",
    "                formatted_cache = []\n",
    "                for i in range(len(raw_cache)):\n",
    "                    layer_key = f\"layer_{i}\"\n",
    "                    if layer_key in raw_cache:\n",
    "                        key_tensor = raw_cache[layer_key][\"key\"].to(self.model.device)\n",
    "                        value_tensor = raw_cache[layer_key][\"value\"].to(self.model.device)\n",
    "                        formatted_cache.append((key_tensor, value_tensor))\n",
    "\n",
    "                self.kv_cache = tuple(formatted_cache)\n",
    "            else:\n",
    "                # If using a simpler format (direct tuple of tensors)\n",
    "                self.kv_cache = cache_data\n",
    "                # Move to device if needed\n",
    "                if hasattr(self.model, 'device'):\n",
    "                    self.kv_cache = tuple((k.to(self.model.device), v.to(self.model.device))\n",
    "                                        for (k, v) in self.kv_cache)\n",
    "\n",
    "            # Validate the KV cache\n",
    "            if not self.validate_kv_cache(self.kv_cache):\n",
    "                print(\"KV cache validation failed. Continuing without cache.\")\n",
    "                self.kv_cache = None\n",
    "                return\n",
    "\n",
    "            print(f\"✅ KV cache loaded successfully with {len(self.kv_cache)} layers!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading KV cache: {e}\")\n",
    "            self.kv_cache = None\n",
    "\n",
    "    def count_tokens(self, text):\n",
    "        \"\"\"Count the number of tokens in text.\"\"\"\n",
    "        return len(self.tokenizer.encode(text))\n",
    "\n",
    "    def add_message(self, role, content):\n",
    "        \"\"\"Add a message to the conversation history.\"\"\"\n",
    "        self.conversation_history.append({\"role\": role, \"content\": content})\n",
    "\n",
    "        if not (role == \"assistant\" and content.startswith(\"[SUMMARY:\")):\n",
    "            self.complete_history.append({\"role\": role, \"content\": content})\n",
    "\n",
    "    def format_conversation(self, custom_system_prompt=None):\n",
    "        \"\"\"Format the conversation history using Llama-3's chat template.\"\"\"\n",
    "        system_prompt = custom_system_prompt if custom_system_prompt is not None else self.system_prompt\n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "        messages.extend(self.conversation_history)\n",
    "        return self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "    def generate_response(self, prompt, max_new_tokens=512):\n",
    "        \"\"\"Generate a response using the model.\"\"\"\n",
    "        prompt_inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=False,  # No padding\n",
    "            truncation=True,\n",
    "            return_attention_mask=True\n",
    "        ).to(self.model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                if self.kv_cache is not None:\n",
    "                    # For CAG: Use the precomputed document cache as context\n",
    "                    outputs = self.model.generate(\n",
    "                        input_ids=prompt_inputs.input_ids,\n",
    "                        attention_mask=prompt_inputs.attention_mask,\n",
    "                        past_key_values=self.kv_cache,\n",
    "                        max_new_tokens=max_new_tokens,\n",
    "                        temperature=0.7,\n",
    "                        top_p=0.9,\n",
    "                        repetition_penalty=1.1,\n",
    "                        do_sample=True,\n",
    "                        pad_token_id=self.tokenizer.eos_token_id,\n",
    "                        eos_token_id=self.tokenizer.eos_token_id,\n",
    "                        use_cache=True\n",
    "                    )\n",
    "                else:\n",
    "                    # Standard generation without KV cache\n",
    "                    outputs = self.model.generate(\n",
    "                        input_ids=prompt_inputs.input_ids,\n",
    "                        attention_mask=prompt_inputs.attention_mask,\n",
    "                        max_new_tokens=max_new_tokens,\n",
    "                        temperature=0.7,\n",
    "                        top_p=0.9,\n",
    "                        repetition_penalty=1.1,\n",
    "                        do_sample=True,\n",
    "                        pad_token_id=self.tokenizer.eos_token_id,\n",
    "                        eos_token_id=self.tokenizer.eos_token_id\n",
    "                    )\n",
    "\n",
    "                # Safety check for output size\n",
    "                if outputs.shape[1] <= prompt_inputs.input_ids.shape[1]:\n",
    "                    return \"I'm having trouble generating a response. Could you try asking again?\"\n",
    "\n",
    "                # Extract only the newly generated tokens\n",
    "                response = self.tokenizer.decode(\n",
    "                    outputs[0, prompt_inputs.input_ids.shape[1]:],\n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "\n",
    "                return response.strip()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in generation: {e}\")\n",
    "                # Fallback response in case of error\n",
    "                return \"I'm having trouble generating a response right now. Could we try a different approach?\"\n",
    "\n",
    "    def summarize_conversation_segment(self, segment):\n",
    "        \"\"\"Summarize conversation segment preserving emotional content.\"\"\"\n",
    "        \n",
    "\n",
    "        conversation_text = \"\"\n",
    "        for msg in segment:\n",
    "            role = \"User\" if msg[\"role\"] == \"user\" else \"Assistant\"\n",
    "            conversation_text += f\"{role}: {msg['content']}\\n\\n\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=\"claude-3-opus-20240229\",\n",
    "                max_tokens=4000,\n",
    "                temperature=0.7,\n",
    "                # This goes in the system parameter\n",
    "                system= f\"\"\"Your task is to create a FACTUAL summary of a therapeutic conversation. Follow these strict guidelines:\n",
    "\n",
    "1. ONLY include information EXPLICITLY stated in the conversation text\n",
    "2. NEVER add details, interpretations, or assumptions not in the original text\n",
    "3. DO NOT infer emotions or thoughts unless directly expressed by the user\n",
    "4. Use NEUTRAL language that accurately reflects what was discussed\n",
    "5. Focus primarily on what the user shared, their concerns and needs\n",
    "6. Maintain EXACT accuracy regarding any mentioned events, feelings, or details\n",
    "7. If something is unclear in the original text, reflect that ambiguity in the summary\n",
    "8. Create a concise but factually complete record of the conversation\n",
    "\n",
    "Remember, your summary should be a condensed version of what was actually said, that mean the total number of tokens should be less than the input you got, with no added content.\"\"\",\n",
    "                messages=[{\"role\": \"user\", \"content\": f\"Please create a factual summary of this therapy conversation text, including ONLY information explicitly stated:\\n\\n{conversation_text}\"}\n",
    "                ]\n",
    "            )\n",
    "            summary = response.content[0].text\n",
    "            return summary\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating summary: {e}\")\n",
    "            return \"[Conversation summary]\"\n",
    "\n",
    "    def create_final_summary(self, collection_name=\"therapy_sessions_v2\"):\n",
    "        \"\"\"Create a comprehensive JSON summary with metadata when the therapy chat ends.\n",
    "\n",
    "        Uses the complete conversation history (excluding summary messages) to generate\n",
    "        a detailed analysis of the therapy session, including key emotions, topics,\n",
    "        actions, suggestions, and an intensity score.\n",
    "\n",
    "        Returns:\n",
    "            str: A JSON-formatted string containing the session summary and metadata.\n",
    "        \"\"\"\n",
    "        # Format the complete conversation history\n",
    "        conversation_text = \"\"\n",
    "        for msg in self.complete_history:\n",
    "            role = \"User\" if msg[\"role\"] == \"user\" else \"Assistant\"\n",
    "            conversation_text += f\"{role}: {msg['content']}\\n\\n\"\n",
    "\n",
    "        try:\n",
    "            # Use Claude API for comprehensive summary with metadata\n",
    "            response = self.client.messages.create(\n",
    "                model=\"claude-3-opus-20240229\",\n",
    "                temperature=0.7,\n",
    "                max_tokens = 4000,\n",
    "                system=\"\"\"You are a specialized AI that creates detailed summaries of therapy conversations for a retrieval system.\n",
    "Review this conversation and provide:\n",
    "\n",
    "1. A comprehensive summary capturing all key issues, insights, and progress. Scale the length based on conversation complexity - longer and more detailed for complex conversations, briefer for simple ones.\n",
    "\n",
    "2. Key emotions expressed by the user (list all relevant emotions)\n",
    "\n",
    "3. Main topics discussed (list all important topics)\n",
    "\n",
    "4. User actions during the conversation (list all significant actions)\n",
    "\n",
    "5. Therapeutic suggestions or techniques offered (list all suggestions made)\n",
    "\n",
    "6. An intensity score from 1-10 that reflects the emotional intensity of the conversation, where:\n",
    "   - 1-3: Mild emotional intensity, mostly informational\n",
    "   - 4-6: Moderate emotional engagement, some vulnerability\n",
    "   - 7-8: High emotional disclosure, significant distress\n",
    "   - 9-10: Crisis-level intensity, acute distress\n",
    "\n",
    "Format your response as JSON matching this schema:\n",
    "{\n",
    "  \"summary\": \"...\",\n",
    "  \"emotions\": [\"...\", \"...\"],\n",
    "  \"topics\": [\"...\", \"...\"],\n",
    "  \"user_actions\": [\"...\", \"...\"],\n",
    "  \"assistant_suggestions\": [\"...\", \"...\"],\n",
    "  \"intensity_score\": X\n",
    "}\"\"\",\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": f\"Create a complete summary with metadata for this entire therapy session:\\n\\n{conversation_text}\"}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            summary_text = response.content[0].text\n",
    "            summary_json = extract_json_from_text(summary_text)\n",
    "\n",
    "            self._save_to_chromadb(summary_json, collection_name)\n",
    "            self._save_to_local_file(summary_json)\n",
    "\n",
    "            return summary_json\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating final summary: {e}\")\n",
    "            return \"{\\\"summary\\\": \\\"Error generating summary\\\", \\\"emotions\\\": [], \\\"topics\\\": [], \\\"user_actions\\\": [], \\\"assistant_suggestions\\\": [], \\\"intensity_score\\\": 1}\"\n",
    "\n",
    "    def _save_to_chromadb(self, summary_json, collection_name=\"therapy_sessions_v2\"):\n",
    "        \"\"\"Internal method to save the session summary to ChromaDB for retrieval.\"\"\"\n",
    "        try:\n",
    "            # Parse the JSON string\n",
    "            summary_data = json.loads(summary_json)\n",
    "\n",
    "            # Generate a unique ID for this session\n",
    "            session_id = f\"session_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "            # Connect to ChromaDB (local by default)\n",
    "            chroma_path = \"/content/drive/MyDrive/therapy_bot_chromadb\"\n",
    "            client = chromadb.PersistentClient(path=chroma_path)\n",
    "\n",
    "            # Get or create the collection\n",
    "            embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "                model_name=\"all-MiniLM-L6-v2\"\n",
    "            )\n",
    "            collection = client.get_or_create_collection(\n",
    "                name=collection_name,\n",
    "                embedding_function=embedding_function\n",
    "            )\n",
    "\n",
    "            # Store the document with metadata\n",
    "            collection.add(\n",
    "                documents=[summary_data[\"summary\"]],\n",
    "                metadatas=[{\n",
    "                    \"emotions\": \", \".join(summary_data[\"emotions\"]),\n",
    "                    \"topics\": \", \".join(summary_data[\"topics\"]),\n",
    "                    \"user_actions\": \", \".join(summary_data[\"user_actions\"]),\n",
    "                    \"assistant_suggestions\": \", \".join(summary_data[\"assistant_suggestions\"]),\n",
    "                    \"intensity_score\": summary_data[\"intensity_score\"],\n",
    "                    \"timestamp\": datetime.datetime.now().isoformat()\n",
    "                }],\n",
    "                ids=[session_id]\n",
    "            )\n",
    "\n",
    "            print(f\"✅ Session saved to ChromaDB with ID: {session_id}\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to ChromaDB: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _save_to_local_file(self, summary_json):\n",
    "        \"\"\"Internal method to save the session summary to a local JSON file.\"\"\"\n",
    "        try:\n",
    "            # Create directory if it doesn't exist\n",
    "            os.makedirs(\"therapy_sessions_v2\", exist_ok=True)\n",
    "\n",
    "            # Generate timestamp-based filename\n",
    "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"therapy_sessions_v2/session_{timestamp}.json\"\n",
    "\n",
    "            # Write to file with nice formatting\n",
    "            with open(filename, \"w\") as f:\n",
    "                # Parse and re-serialize to get pretty formatting\n",
    "                summary_data = json.loads(summary_json)\n",
    "                json.dump(summary_data, f, indent=2)\n",
    "\n",
    "            print(f\"✅ Session saved to file: {filename}\")\n",
    "            return filename\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to local file: {e}\")\n",
    "            return None\n",
    "\n",
    "    def needs_context(self, message):\n",
    "        \"\"\"Determine if the user's message needs context from past sessions.\"\"\"\n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=\"claude-3-opus-20240229\",\n",
    "                temperature=0.7,\n",
    "                max_tokens=4000,\n",
    "                system=\"\"\"Analyze if this message needs context from past therapy sessions. Be strict in your analysis.\n",
    "    Return a JSON object with:\n",
    "    {\n",
    "        \"needs_context\": true/false,\n",
    "        \"context_type\": \"emotional/progress/coping/topical\",\n",
    "        \"reason\": \"brief explanation\",\n",
    "        \"relevance_score\": 0-10,  # How relevant is the context needed?\n",
    "        \"should_use_context\": true/false  # Only true if relevance_score > 7\n",
    "    }\"\"\",\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": f\"\"\"Analyze if this message needs context from past therapy sessions. Be strict.\n",
    "    Message: {message}\n",
    "\n",
    "    Consider:\n",
    "    1. Is there a direct reference to past sessions?\n",
    "    2. Is the context crucial for understanding the current message?\n",
    "    3. Would the response be significantly better with context?\n",
    "    4. Is the user explicitly asking about past interactions?\n",
    "\n",
    "    Only recommend context if it's highly relevant and necessary.\"\"\"}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            analysis = json.loads(response.content[0].text)\n",
    "            return analysis\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing context need: {e}\")\n",
    "            return {\"needs_context\": False, \"context_type\": None, \"reason\": \"Error in analysis\", \"relevance_score\": 0, \"should_use_context\": False}\n",
    "\n",
    "    def retrieve_relevant_context(self, query, context_type=\"emotional\"):\n",
    "        \"\"\"Query ChromaDB for relevant past conversations.\"\"\"\n",
    "        try:\n",
    "            print(f\"\\n🔍 Searching for {context_type} context...\")\n",
    "\n",
    "         \n",
    "            collection = self.chroma_client.get_collection(\n",
    "                name=\"therapy_sessions_v2\",  # New collection name\n",
    "                embedding_function=self.embedding_function\n",
    "            )\n",
    "\n",
    "            \n",
    "            results = collection.query(\n",
    "                query_texts=[query],\n",
    "                n_results=2,  \n",
    "                where={\"intensity_score\": {\"$gte\": 3}}  \n",
    "            )\n",
    "\n",
    "            if results and 'documents' in results and results['documents']:\n",
    "                print(f\"✅ Found {len(results['documents'])} relevant past sessions\")\n",
    "                for i, (doc, metadata_list) in enumerate(zip(results['documents'], results['metadatas'])):\n",
    "                    print(f\"\\nSession {i+1}:\")\n",
    "                    if isinstance(metadata_list, list) and metadata_list:\n",
    "                        metadata = metadata_list[0]\n",
    "                        print(f\"Topics: {metadata.get('topics', 'N/A')}\")\n",
    "                        print(f\"Emotions: {metadata.get('emotions', 'N/A')}\")\n",
    "                        print(f\"Intensity: {metadata.get('intensity_score', 'N/A')}\")\n",
    "                    else:\n",
    "                        print(\"No metadata available\")\n",
    "                return results\n",
    "            else:\n",
    "                print(\"ℹ️ No relevant past sessions found\")\n",
    "                return None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving context: {e}\")\n",
    "            return None\n",
    "\n",
    "    def format_context(self, results):\n",
    "        \"\"\"Format retrieved context for inclusion in the prompt.\"\"\"\n",
    "        if not results or 'documents' not in results or not results['documents']:\n",
    "            return \"\"\n",
    "\n",
    "        context = \"\\nRelevant past sessions:\\n\"\n",
    "        for i, (doc, metadata_list) in enumerate(zip(results['documents'], results['metadatas'])):\n",
    "            context += f\"\\nSession {i+1}:\\n\"\n",
    "            context += f\"Summary: {doc}\\n\"\n",
    "            if isinstance(metadata_list, list) and metadata_list:\n",
    "                # Get the first metadata dictionary from the list\n",
    "                metadata = metadata_list[0]\n",
    "                context += f\"Topics: {metadata.get('topics', 'N/A')}\\n\"\n",
    "                context += f\"Emotions: {metadata.get('emotions', 'N/A')}\\n\"\n",
    "                context += f\"Intensity: {metadata.get('intensity_score', 'N/A')}\\n\"\n",
    "            else:\n",
    "                context += \"No metadata available\\n\"\n",
    "\n",
    "        return context\n",
    "\n",
    "    def manage_context_length(self):\n",
    "        \"\"\"Manage context length by summarizing older parts.\"\"\"\n",
    "        current_prompt = self.format_conversation()\n",
    "        token_count = self.count_tokens(current_prompt)\n",
    "\n",
    "        \n",
    "        if token_count > self.max_tokens * 0.9:\n",
    "            print(f\"Context length ({token_count} tokens) exceeds 90% of maximum allowed. Ending chat.\")\n",
    "            return False\n",
    "\n",
    "       \n",
    "        if token_count > self.max_tokens * 0.6:\n",
    "            print(f\"Context length ({token_count} tokens) approaching limit. Summarizing...\")\n",
    "\n",
    "          \n",
    "            keep_recent = min(2, len(self.conversation_history))\n",
    "\n",
    "            if len(self.conversation_history) <= keep_recent + 2:\n",
    "                return True  \n",
    "\n",
    "           \n",
    "            summarize_end = len(self.conversation_history) - keep_recent\n",
    "            segment_to_summarize = self.conversation_history[:summarize_end]\n",
    "\n",
    "           \n",
    "            summary = self.summarize_conversation_segment(segment_to_summarize)\n",
    "\n",
    "            if summary:\n",
    "                print(f\"\\n>>> SUMMARY CREATED: {summary}\\n\")\n",
    "\n",
    "                summary_message = {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": f\"[SUMMARY: {summary}]\"\n",
    "                }\n",
    "                self.conversation_history = [summary_message] + self.conversation_history[summarize_end:]\n",
    "                self.contains_summary = True\n",
    "\n",
    "        return True \n",
    "\n",
    "    def chat(self, user_message):\n",
    "        \"\"\"Process user message and generate response.\"\"\"\n",
    "        try:\n",
    "            print(\"Processing message...\")\n",
    "\n",
    "           \n",
    "            context_analysis = self.needs_context(user_message)\n",
    "            augmented_system_prompt = self.system_prompt\n",
    "           \n",
    "            if context_analysis[\"needs_context\"] and context_analysis.get(\"should_use_context\", False):\n",
    "                print(f\"Retrieving {context_analysis['context_type']} context: {context_analysis['reason']}\")\n",
    "                retrieved_results = self.retrieve_relevant_context(\n",
    "                    user_message,\n",
    "                    context_type=context_analysis[\"context_type\"]\n",
    "                )\n",
    "                context = self.format_context(retrieved_results)\n",
    "\n",
    "                \n",
    "                if context:\n",
    "                    augmented_system_prompt += f\"\"\"\n",
    "\n",
    "                    PAST SESSION CONTEXT (Use only if directly relevant):\n",
    "                    {context}\n",
    "\n",
    "                    Guidelines for using past context:\n",
    "                    1. When asked \"do you remember me\", respond with specific details from past sessions\n",
    "                    2. Acknowledge the specific topics and emotions discussed\n",
    "                    3. Show continuity in the therapeutic relationship\n",
    "                    4. Be warm and personal in your response\n",
    "                    5. Use the past context to inform your response, but focus on the present\n",
    "\n",
    "                    Example response for \"do you remember me\":\n",
    "                    \"Yes, I remember our previous sessions where we discussed your upcoming surgery and the anxiety you were feeling about it. You were feeling anxious, scared, and overwhelmed at that time. How are you feeling about the surgery now? Has anything changed since we last spoke?\"\n",
    "                    \"\"\"\n",
    "            #     else:\n",
    "            #         augmented_system_prompt = self.system_prompt\n",
    "            # else:\n",
    "            #     augmented_system_prompt = self.system_prompt\n",
    "\n",
    "            test_history = self.conversation_history.copy()\n",
    "            test_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "            test_prompt = self.tokenizer.apply_chat_template(\n",
    "                [{\"role\": \"system\", \"content\": augmented_system_prompt}] + test_history,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "\n",
    "            if self.count_tokens(test_prompt) > self.max_tokens * 0.9:\n",
    "                return \"I apologize, but our conversation has reached its length limit. Let's start a new chat to continue.\"\n",
    "\n",
    "            \n",
    "            self.add_message(\"user\", user_message)\n",
    "\n",
    "        \n",
    "            if not self.manage_context_length():\n",
    "                return \"I apologize, but our conversation has reached its length limit. Let's start a new chat to continue.\"\n",
    "\n",
    "           \n",
    "            prompt = self.format_conversation(custom_system_prompt=augmented_system_prompt)\n",
    "            response = self.generate_response(prompt)\n",
    "            self.add_message(\"assistant\", response)\n",
    "\n",
    "            # Check again after adding response\n",
    "            if not self.manage_context_length():\n",
    "                return response + \"\\n\\nI apologize, but our conversation has reached its length limit. Let's start a new chat to continue.\"\n",
    "\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Error in chat: {e}\")\n",
    "            return \"I apologize, but I encountered an error. Let's try again.\"\n",
    "\n",
    "def run_therapy_chat(kv_cache_path=None):\n",
    "    \"\"\"Run the therapy chatbot.\"\"\"\n",
    "    print(\"Initializing Llama-3 Therapy Bot...\")\n",
    "    bot = Llama3TherapyBot(kv_cache_path=kv_cache_path)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Llama-3 Therapy Bot initialized!\")\n",
    "    if kv_cache_path:\n",
    "        print(f\"Using KV cache from: {kv_cache_path}\")\n",
    "    print(\"Type 'exit', 'quit', or 'bye' to end the conversation.\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "    print(\"Therapy Bot: Hello, I'm here to support you. What's on your mind today?\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \").strip()\n",
    "        if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "            print(\"\\nTherapy Bot: Take care. I'm here if you need to talk again.\")\n",
    "            final_summary = bot.create_final_summary()\n",
    "            print(\"\\nGenerating session summary...\")\n",
    "\n",
    "            # Here you would store final_summary in ChromaDB\n",
    "            print(f\"\\nSession Summary: {final_summary}\")\n",
    "            break\n",
    "        try:\n",
    "            response = bot.chat(user_input)\n",
    "            print(f\"\\nTherapy Bot: {response}\")\n",
    "\n",
    "            # Check if we need to end due to context length\n",
    "            if \"conversation has reached its length limit\" in response:\n",
    "                print(\"\\nChat ended due to context length limitations.\")\n",
    "                final_summary = bot.create_final_summary()\n",
    "                print(\"\\nGenerating session summary...\")\n",
    "                print(f\"\\nSession Summary: {final_summary}\")\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            print(\"\\nTherapy Bot: Let's try that again.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_therapy_chat(kv_cache_path=\"mistral_cag_knowledge_small (2)_llama_1b_kv_cache.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dg1AMUlL1lCS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KLfR69bh1lDs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "cfJzXkieMxFv",
    "PiC9blVEXfsx",
    "V6iSTAxCd_fM",
    "JBxipAerOXC7"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
